# Environment Configuration for CSV Ingestion Pipeline
#
# Copy this file to .env and fill in your actual values
# These environment variables replace Airflow Connections and Variables

# ============================================================================
# Kafka Configuration
# ============================================================================

# Bootstrap servers (comma-separated for multiple brokers)
KAFKA_BOOTSTRAP_SERVERS=openhouse-kafka:9092

# Topic to listen for CSV ingestion events
KAFKA_TOPIC_CSV_INGESTION=csv-ingestion

# Consumer group ID for offset tracking
KAFKA_CONSUMER_GROUP_ID=airflow-csv-ingestion-consumer

# SASL Authentication
KAFKA_SASL_MECHANISM=PLAIN
KAFKA_SASL_USERNAME=admin
KAFKA_SASL_PASSWORD=admin

# Security protocol (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL)
KAFKA_SECURITY_PROTOCOL=SASL_PLAINTEXT

# Consumer behavior
KAFKA_AUTO_OFFSET_RESET=latest
KAFKA_POLL_TIMEOUT=10
KAFKA_POKE_INTERVAL=30
KAFKA_SESSION_TIMEOUT_MS=30000

# ============================================================================
# NiFi Configuration
# ============================================================================

# NiFi API base URL
NIFI_API_BASE_URL=https://openhouse-nifi:8443/nifi-api

# Process Group ID to trigger
NIFI_PROCESS_GROUP_ID=4be3c5be-019b-1000-4ef1-949cbb8c08de

# SSL verification (true/false)
NIFI_VERIFY_SSL=false

# Request timeout in seconds
NIFI_REQUEST_TIMEOUT=30

# ============================================================================
# Deployment Notes
# ============================================================================
#
# Local Development:
#   export $(cat .env | xargs)
#   python airflow/dags/csv_ingestion_listener_dag.py
#
# Kubernetes:
#   Create ConfigMap: kubectl create configmap airflow-config --from-env-file=.env
#   Create Secret: kubectl create secret generic airflow-secrets --from-env-file=.env.secrets
#   Reference in deployment YAML using envFrom
#
