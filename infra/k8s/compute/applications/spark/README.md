# Spark Applications

Kubernetes manifests for Spark applications running on Spark Operator.

## Structure

```
spark/
└── legacy/               # Legacy Spark jobs
    └── taxi-data-ingestion.yaml
```

## Legacy Job: Taxi Data Ingestion

**Purpose**: Ingest taxi trip data from MinIO raw bucket into Lakekeeper bronze warehouse

**Job Details**:

- **Input**: Parquet files from `s3a://raw/taxi/*/*.parquet` (generated by NiFi)
- **Output**: Iceberg table `taxi_trips` in `bronze` warehouse
- **Processing**: Reads Parquet chunks, performs data cleaning and deduplication, writes to Iceberg format

## Prerequisites

1. **Spark Operator** installed and running
2. **Lakekeeper** catalog service accessible at `http://openhouse-lakekeeper:8181/catalog`
3. **MinIO** object storage accessible at `http://openhouse-minio:9000`
4. **Keycloak** OAuth2 service for authentication
5. **Docker image** built and pushed to registry:
   - Image: `hungvt0110/ingestion-job:latest`
   - Build script: See `spark-jobs/build-image.sh`
6. **Service Account**: `openhouse-spark-operator-spark` with proper RBAC permissions

## Grant Lakekeeper Access (First Run Only)

**Important**: On the first job run, the job will fail because the Spark service account does not have access to the Lakekeeper warehouse. Follow these steps to grant access:

**Step 1: Get Service Account ID**

Go to Lakekeeper UI → Server Settings → Users → Copy the ID of `service-account-spark`:

![Service Account Spark](../../../../../assets/service-account-spark.png)

**Step 2: Grant Warehouse Permissions**

Navigate to the warehouse you want to access (e.g., `bronze`) → Permissions → Grant:

1. Tick **by Id**
2. Paste the service account ID you copied
3. Select **ownership** role
4. Click **SAVE**

![Grant Spark Permissions - Step 1](../../../../../assets/grant-spark-2.png)

![Grant Spark Permissions - Step 2](../../../../../assets/grant-spark.png)

**Step 3: Run Job Again**

After granting permissions, run the Spark job normally. It should now succeed.

## Deployment

Deploy the taxi data ingestion job manually:

```bash
# Deploy the job
kubectl apply -f legacy/taxi-data-ingestion.yaml

# Check job status
kubectl get sparkapplication taxi-data-ingestion -n default

# Monitor driver logs
kubectl logs -l spark-role=driver,spark-app-name=taxi-data-ingestion -n default -f

# Check all Spark pods
kubectl get pods -l app=taxi-data-ingestion -n default

# Delete the job
kubectl delete -f legacy/taxi-data-ingestion.yaml
```

## Configuration

### Job Specifications

**Environment Variables** (configured in `taxi-data-ingestion.yaml`):

- `SPARK_MINOR_VERSION`: 3.5
- `ICEBERG_VERSION`: 1.5.2
- `CATALOG_URL`: `http://openhouse-lakekeeper:8181/catalog`
- `CLIENT_ID`: `spark`
- `CLIENT_SECRET`: `3FfkvrupMYsojoT2RnXqknvjCsljwFWl`
- `WAREHOUSE`: `bronze`
- `KEYCLOAK_TOKEN_ENDPOINT`: `http://openhouse-keycloak:80/realms/iceberg/protocol/openid-connect/token`
- `AWS_ACCESS_KEY_ID`: `admin`
- `AWS_SECRET_ACCESS_KEY`: `admin123`

**Resources**:

- **Driver**: 1 core, 2GB memory (512MB overhead)
- **Executors**: 2 cores, 4GB memory (1GB overhead), 3 instances

**Spark Configuration**:

- Iceberg runtime and AWS bundles auto-loaded via `spark.jars.packages`
- S3A filesystem configured for MinIO access
- Ivy cache set to `/tmp/.ivy2` for pod write access

## Troubleshooting

**Job not starting:**

```bash
# Check Spark Operator is running
kubectl get pods -l app.kubernetes.io/name=spark-operator -n spark-operator

# Describe the SparkApplication
kubectl describe sparkapplication taxi-data-ingestion -n default

# Check service account permissions
kubectl get serviceaccount openhouse-spark-operator-spark -n default
```

**Authentication errors:**

```bash
# Verify Lakekeeper is accessible
kubectl get svc openhouse-lakekeeper -n default

# Check Keycloak endpoint
kubectl get svc openhouse-keycloak -n default

# Test OAuth2 token endpoint
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -X POST http://openhouse-keycloak:80/realms/iceberg/protocol/openid-connect/token \
  -d "client_id=spark" \
  -d "client_secret=3FfkvrupMYsojoT2RnXqknvjCsljwFWl" \
  -d "grant_type=client_credentials"
```

**MinIO connection issues:**

```bash
# Verify MinIO is accessible
kubectl get svc openhouse-minio -n default

# Check if raw bucket exists and has data
kubectl run -it --rm aws-cli --image=amazon/aws-cli --restart=Never -- \
  s3 ls s3://raw/taxi/ \
  --endpoint-url http://openhouse-minio:9000 \
  --no-verify-ssl
```

**Driver/Executor pod failures:**

```bash
# Check driver pod logs
kubectl logs -l spark-role=driver,spark-app-name=taxi-data-ingestion -n default

# Check executor pod logs
kubectl logs -l spark-role=executor,spark-app-name=taxi-data-ingestion -n default

# Check pod events
kubectl get events -n default --sort-by='.lastTimestamp' | grep taxi-data-ingestion
```
