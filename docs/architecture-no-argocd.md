# Data Lakehouse Architecture Without ArgoCD
## Design, Flow & Implementation Plan

---

## ðŸ“‹ Executive Summary

**Architecture:** Airflow + Kafka + NiFi + Spark + MinIO + Lakekeeper (NO ArgoCD)

**Core Flow:**
```
File Upload â†’ Kafka â†’ Airflow Listener
                        â”œâ”€ Task 1: Trigger NiFi via REST API
                        â”œâ”€ Task 2: Monitor NiFi ingestion
                        â”œâ”€ Task 3: Submit Spark job (raw â†’ bronze)
                        â””â”€ Task 4: Monitor Spark job completion
```

**Key Decision:** Airflow submits Spark jobs **directly via kubectl apply** (no Git, no ArgoCD)

---

## 1. Architecture Overview

### 1.1 System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER LAYER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Web UI / API: File Upload Interface                              â”‚
â”‚ â€¢ Datasource API (FastAPI): Accept file uploads                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP POST /upload
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION LAYER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚ â”‚  Apache Kafka    â”‚â”€â”€â”€â”€â–¶â”‚  Airflow Listenerâ”‚                     â”‚
â”‚ â”‚  (Orchestration) â”‚     â”‚  (Trigger DAG)   â”‚                     â”‚
â”‚ â”‚                  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚ â”‚ Topics:          â”‚              â”‚                                â”‚
â”‚ â”‚ â€¢ file-uploaded  â”‚              â–¼                                â”‚
â”‚ â”‚ â€¢ raw-data-ready â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Airflow DAG       â”‚                 â”‚
â”‚         â–²                â”‚ (data_ingestion.py) â”‚                 â”‚
â”‚         â”‚                â”‚                     â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ Tasks:              â”‚                 â”‚
â”‚ â”‚  Datasource API  â”‚     â”‚ 1. trigger_nifi    â”‚                 â”‚
â”‚ â”‚  (FastAPI)       â”‚     â”‚ 2. wait_nifi_done  â”‚                 â”‚
â”‚ â”‚                  â”‚     â”‚ 3. submit_spark    â”‚                 â”‚
â”‚ â”‚ POST /upload     â”‚     â”‚ 4. wait_spark      â”‚                 â”‚
â”‚ â”‚ - Store file     â”‚     â”‚ 5. publish_success â”‚                 â”‚
â”‚ â”‚ - Send to Kafka  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                               â”‚
â”‚                                   â–¼                               â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                        â”‚   Apache NiFi    â”‚                       â”‚
â”‚                        â”‚ (Data Ingest)    â”‚                       â”‚
â”‚                        â”‚                  â”‚                       â”‚
â”‚                        â”‚ Processors:      â”‚                       â”‚
â”‚                        â”‚ â€¢ ConsumeKafka   â”‚                       â”‚
â”‚                        â”‚ â€¢ ValidateRecord â”‚                       â”‚
â”‚                        â”‚ â€¢ TransformJSON  â”‚                       â”‚
â”‚                        â”‚ â€¢ PutS3Object    â”‚                       â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                 â”‚                                â”‚
â”‚                                 â–¼                                â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                      â”‚  MinIO (S3 API)  â”‚                        â”‚
â”‚                      â”‚                  â”‚                        â”‚
â”‚                      â”‚ Buckets:         â”‚                        â”‚
â”‚                      â”‚ â€¢ raw/           â”‚                        â”‚
â”‚                      â”‚ â€¢ warehouse/     â”‚                        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMATION LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Airflow Task 3: Submit Spark Job                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ kubectl apply -f spark-bronze-job.yaml         â”‚               â”‚
â”‚  â”‚ (No Git, Direct to K8s)                         â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â”‚                                          â”‚
â”‚                         â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚    Spark Application (Spark Operator)          â”‚               â”‚
â”‚  â”‚                                                â”‚               â”‚
â”‚  â”‚  Job: bronze_transform.py                      â”‚               â”‚
â”‚  â”‚  Input: s3a://raw/*.parquet                    â”‚               â”‚
â”‚  â”‚  Output: Iceberg tables (bronze warehouse)     â”‚               â”‚
â”‚  â”‚                                                â”‚               â”‚
â”‚  â”‚  Tasks:                                        â”‚               â”‚
â”‚  â”‚  1. Read from MinIO raw bucket                 â”‚               â”‚
â”‚  â”‚  2. Validate data quality                      â”‚               â”‚
â”‚  â”‚  3. Transform to standard schema               â”‚               â”‚
â”‚  â”‚  4. Write to Iceberg Bronze warehouse          â”‚               â”‚
â”‚  â”‚  5. Register table in Lakekeeper               â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  MinIO (S3 API)      â”‚  â”‚  Lakekeeper          â”‚              â”‚
â”‚  â”‚                      â”‚  â”‚  (Iceberg Catalog)   â”‚              â”‚
â”‚  â”‚ Buckets:             â”‚  â”‚                      â”‚              â”‚
â”‚  â”‚ â€¢ raw/               â”‚  â”‚ Warehouses:          â”‚              â”‚
â”‚  â”‚   â””â”€ orders.parquet  â”‚  â”‚ â€¢ bronze_warehouse   â”‚              â”‚
â”‚  â”‚   â””â”€ customers.par   â”‚  â”‚   â””â”€ orders (table)  â”‚              â”‚
â”‚  â”‚                      â”‚  â”‚   â””â”€ customers       â”‚              â”‚
â”‚  â”‚ â€¢ warehouse/         â”‚  â”‚ â€¢ silver_warehouse   â”‚              â”‚
â”‚  â”‚   â””â”€ bronze/         â”‚  â”‚ â€¢ gold_warehouse     â”‚              â”‚
â”‚  â”‚   â””â”€ silver/         â”‚  â”‚                      â”‚              â”‚
â”‚  â”‚   â””â”€ gold/           â”‚  â”‚ Features:            â”‚              â”‚
â”‚  â”‚                      â”‚  â”‚ â€¢ Time travel        â”‚              â”‚
â”‚  â”‚ Features:            â”‚  â”‚ â€¢ Schema evolution   â”‚              â”‚
â”‚  â”‚ â€¢ S3-compatible      â”‚  â”‚ â€¢ ACID transactions  â”‚              â”‚
â”‚  â”‚ â€¢ Multi-tenancy      â”‚  â”‚ â€¢ Data versioning    â”‚              â”‚
â”‚  â”‚ â€¢ ACL/Policy         â”‚  â”‚                      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚     PostgreSQL (Metadata Storage)            â”‚                â”‚
â”‚  â”‚                                              â”‚                â”‚
â”‚  â”‚  Databases:                                  â”‚                â”‚
â”‚  â”‚  â€¢ airflow_db (Airflow metadata)             â”‚                â”‚
â”‚  â”‚  â€¢ data_catalog (Custom metadata)            â”‚                â”‚
â”‚  â”‚    â””â”€ tables, lineage, metrics               â”‚                â”‚
â”‚  â”‚                                              â”‚                â”‚
â”‚  â”‚  Tables:                                     â”‚                â”‚
â”‚  â”‚  â€¢ dag_runs, task_instances                  â”‚                â”‚
â”‚  â”‚  â€¢ table_lineage, data_quality               â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Component Roles

| Component | Role | Responsibility |
|-----------|------|-----------------|
| **Datasource API** | Entry Point | Accept file uploads, publish to Kafka |
| **Kafka** | Message Bus | Orchestrate workflow via events |
| **Airflow** | Orchestrator | Listen to events, trigger workflows, submit jobs |
| **NiFi** | Data Ingestion | Read from sources, validate, transform, load to MinIO |
| **MinIO** | Data Lake | Store raw data & Iceberg warehouse |
| **Spark** | Transformation | Transform raw â†’ bronze (Iceberg) |
| **Lakekeeper** | Catalog | Manage Iceberg table metadata |
| **PostgreSQL** | Metadata Store | Airflow metadata + custom lineage |

---

## 2. Detailed Flow

### 2.1 Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: FILE UPLOAD & KAFKA PUBLISH                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User / API Client
    â”‚
    â””â”€ POST /api/upload
       â”œâ”€ file: orders_001.csv
       â””â”€ format: CSV
            â”‚
            â–¼
    Datasource API (FastAPI)
    â”œâ”€ Receive file
    â”œâ”€ Store to temp location
    â”œâ”€ Create metadata:
    â”‚  â”œâ”€ filename: orders_001.csv
    â”‚  â”œâ”€ size: 1.2 MB
    â”‚  â”œâ”€ content_type: text/csv
    â”‚  â”œâ”€ timestamp: 2025-01-15T14:30:00Z
    â”‚  â””â”€ upload_id: upload-20250115-001
    â”‚
    â”œâ”€ Publish to Kafka topic: file-uploaded
    â”‚  Message:
    â”‚  {
    â”‚    "filename": "orders_001.csv",
    â”‚    "upload_path": "s3a://uploads/orders_001.csv",
    â”‚    "size": 1248576,
    â”‚    "content_type": "text/csv",
    â”‚    "timestamp": "2025-01-15T14:30:00Z",
    â”‚    "upload_id": "upload-20250115-001"
    â”‚  }
    â”‚
    â””â”€ Return HTTP 202 Accepted

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: AIRFLOW LISTENS TO KAFKA & TRIGGERS WORKFLOW           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow Sensor (Runs continuously)
    â”‚
    â”œâ”€ Listen to Kafka topic: file-uploaded
    â”œâ”€ Detect new message from Datasource API
    â”‚
    â””â”€ TRIGGER: data_ingestion DAG
       â”œâ”€ Passed parameters:
       â”‚  â”œâ”€ filename: orders_001.csv
       â”‚  â”œâ”€ upload_path: s3a://uploads/orders_001.csv
       â”‚  â””â”€ upload_id: upload-20250115-001
       â”‚
       â””â”€ DAG RUN ID: data-ingestion-20250115-143000-001

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: AIRFLOW DAG TASK 1 - TRIGGER NIFI INGEST              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow DAG: data_ingestion (run_id: 20250115-143000-001)
    â”‚
    â””â”€ Task 1: trigger_nifi_flow
       â”œâ”€ Extract parameters:
       â”‚  â””â”€ filename: orders_001.csv
       â”‚
       â”œâ”€ Call NiFi REST API:
       â”‚  POST http://nifi:8080/nifi-api/process-groups/xyz/start
       â”‚  Body:
       â”‚  {
       â”‚    "processorGroupId": "data-ingestion-pg",
       â”‚    "variables": {
       â”‚      "input_file": "s3a://uploads/orders_001.csv",
       â”‚      "output_bucket": "s3a://raw/",
       â”‚      "data_type": "orders"
       â”‚    }
       â”‚  }
       â”‚
       â”œâ”€ NiFi Response: 200 OK
       â”‚  â””â”€ flow_run_id: nifi-flow-20250115-001
       â”‚
       â”œâ”€ Save to XCom:
       â”‚  â””â”€ nifi_flow_run_id: nifi-flow-20250115-001
       â”‚
       â””â”€ Task Status: SUCCESS âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: NIFI INGESTS DATA (PARALLEL EXECUTION)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NiFi Process Group: DataIngestionFlow
    â”‚
    â”œâ”€ Processor 1: ConsumeKafka
    â”‚  â””â”€ Read from Kafka topic: file-uploaded
    â”‚     â”œâ”€ Consume message from Step 1
    â”‚     â””â”€ Extract: upload_path, filename, timestamp
    â”‚
    â”œâ”€ Processor 2: FetchS3Object
    â”‚  â””â”€ Fetch file from upload location
    â”‚     â”œâ”€ Read: s3a://uploads/orders_001.csv
    â”‚     â”œâ”€ Parse as CSV
    â”‚     â””â”€ Preview: 1000 rows
    â”‚
    â”œâ”€ Processor 3: ValidateRecord
    â”‚  â”œâ”€ Schema validation
    â”‚  â”‚  â”œâ”€ Required columns: order_id, customer_id, amount, date
    â”‚  â”‚  â”œâ”€ Data types: int, int, decimal, date
    â”‚  â”‚  â””â”€ Not null checks
    â”‚  â”‚
    â”‚  â””â”€ Route:
    â”‚     â”œâ”€ Valid records â†’ Next processor (95%)
    â”‚     â””â”€ Invalid records â†’ Error bucket (5%)
    â”‚
    â”œâ”€ Processor 4: UpdateAttribute
    â”‚  â”œâ”€ Add processing metadata:
    â”‚  â”‚  â”œâ”€ ingest_timestamp: 2025-01-15T14:30:05Z
    â”‚  â”‚  â”œâ”€ processing_date: 2025-01-15
    â”‚  â”‚  â”œâ”€ data_version: v1
    â”‚  â”‚  â”œâ”€ source_system: datasource-api
    â”‚  â”‚  â””â”€ record_count: 1000
    â”‚  â”‚
    â”‚  â””â”€ Set filename for output
    â”‚
    â”œâ”€ Processor 5: ConvertRecord
    â”‚  â”œâ”€ Input format: CSV
    â”‚  â”œâ”€ Output format: Parquet
    â”‚  â”‚  (Better compression, nested schema support for Iceberg)
    â”‚  â”‚
    â”‚  â””â”€ Schema mapping:
    â”‚     â”œâ”€ order_id â†’ order_id (BIGINT)
    â”‚     â”œâ”€ customer_id â†’ customer_id (BIGINT)
    â”‚     â”œâ”€ amount â†’ amount (DECIMAL(10,2))
    â”‚     â”œâ”€ date â†’ order_date (DATE)
    â”‚     â””â”€ metadata â†’ _metadata (STRUCT)
    â”‚
    â”œâ”€ Processor 6: PutS3Object
    â”‚  â”œâ”€ Write to MinIO
    â”‚  â”œâ”€ S3 URI: s3a://raw/orders/2025-01-15/orders_001.parquet
    â”‚  â”‚  (Partitioned by date for faster queries)
    â”‚  â”‚
    â”‚  â”œâ”€ Success: Write to Kafka topic: raw-data-ready
    â”‚  â”‚  Message:
    â”‚  â”‚  {
    â”‚  â”‚    "file_path": "s3a://raw/orders/2025-01-15/orders_001.parquet",
    â”‚  â”‚    "data_type": "orders",
    â”‚  â”‚    "record_count": 1000,
    â”‚  â”‚    "processing_date": "2025-01-15",
    â”‚  â”‚    "ingest_completed_at": "2025-01-15T14:30:15Z",
    â”‚  â”‚    "upload_id": "upload-20250115-001"
    â”‚  â”‚  }
    â”‚  â”‚
    â”‚  â””â”€ Failure: Route to error handling
    â”‚
    â””â”€ Duration: ~10 seconds

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: AIRFLOW TASK 2 - WAIT FOR NIFI COMPLETION             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow Task 2: wait_nifi_completion
    â”‚
    â”œâ”€ Method 1: Monitor Kafka topic: raw-data-ready
    â”‚  â”œâ”€ Wait for message matching upload_id
    â”‚  â”œâ”€ Timeout: 5 minutes
    â”‚  â”œâ”€ Extract from message:
    â”‚  â”‚  â”œâ”€ file_path: s3a://raw/orders/2025-01-15/orders_001.parquet
    â”‚  â”‚  â”œâ”€ record_count: 1000
    â”‚  â”‚  â””â”€ processing_date: 2025-01-15
    â”‚  â”‚
    â”‚  â””â”€ Save to XCom:
    â”‚     â”œâ”€ raw_file_path: s3a://raw/orders/2025-01-15/orders_001.parquet
    â”‚     â””â”€ record_count: 1000
    â”‚
    â””â”€ Task Status: SUCCESS âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: AIRFLOW TASK 3 - SUBMIT SPARK JOB                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow Task 3: submit_spark_job
    â”‚
    â”œâ”€ Load Spark job template:
    â”‚  â””â”€ Template: spark-bronze-transform.yaml
    â”‚
    â”œâ”€ Render template with parameters:
    â”‚  â”œâ”€ job_id: bronze-dag-20250115-143000-001
    â”‚  â”œâ”€ input_path: s3a://raw/orders/2025-01-15/orders_001.parquet
    â”‚  â”œâ”€ output_warehouse: bronze_warehouse
    â”‚  â”œâ”€ table_name: orders
    â”‚  â”œâ”€ processing_date: 2025-01-15
    â”‚  â”œâ”€ lakekeeper_uri: http://lakekeeper:8080
    â”‚  â”œâ”€ minio_endpoint: http://minio:9000
    â”‚  â”œâ”€ driver_cores: 2
    â”‚  â”œâ”€ driver_memory: 2Gi
    â”‚  â”œâ”€ executor_cores: 2
    â”‚  â”œâ”€ executor_instances: 3
    â”‚  â””â”€ executor_memory: 4Gi
    â”‚
    â”œâ”€ Write manifest to temp file:
    â”‚  â””â”€ /tmp/spark-bronze-job-20250115-143000-001.yaml
    â”‚
    â”œâ”€ Submit to Kubernetes:
    â”‚  â””â”€ kubectl apply -f /tmp/spark-bronze-job-20250115-143000-001.yaml
    â”‚     -n data-platform
    â”‚
    â”œâ”€ Verify submission:
    â”‚  â”œâ”€ Check if SparkApplication was created
    â”‚  â”œâ”€ Get job_id from status
    â”‚  â””â”€ Save to XCom:
    â”‚     â””â”€ spark_job_id: bronze-dag-20250115-143000-001
    â”‚
    â””â”€ Task Status: SUCCESS âœ… (submission, not completion)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: SPARK JOB EXECUTES (TRANSFORMATION)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kubernetes Spark Operator
    â”‚
    â””â”€ Create SparkApplication: bronze-dag-20250115-143000-001
       â”œâ”€ Driver Pod (1 pod):
       â”‚  â”œâ”€ Cores: 2
       â”‚  â”œâ”€ Memory: 2Gi
       â”‚  â””â”€ Status: Running
       â”‚
       â””â”€ Executor Pods (3 pods):
          â”œâ”€ Pod 1: executor-1
          â”‚  â”œâ”€ Cores: 2 each = 6 total cores
          â”‚  â”œâ”€ Memory: 4Gi each = 12Gi total memory
          â”‚  â””â”€ Status: Running
          â”‚
          â”œâ”€ Pod 2: executor-2
          â””â”€ Pod 3: executor-3
             â””â”€ Status: All Running âœ…

Spark Application: bronze_transform.py
    â”‚
    â”œâ”€ STEP 7.1: Read from MinIO
    â”‚  â”‚
    â”‚  â”œâ”€ Connect to MinIO (S3-compatible)
    â”‚  â”‚  â”œâ”€ Endpoint: http://minio:9000
    â”‚  â”‚  â”œâ”€ Access Key: XXXX
    â”‚  â”‚  â””â”€ Secret Key: XXXX
    â”‚  â”‚
    â”‚  â”œâ”€ Read parquet file:
    â”‚  â”‚  â””â”€ s3a://raw/orders/2025-01-15/orders_001.parquet
    â”‚  â”‚
    â”‚  â””â”€ Load into DataFrame (DataFrame API)
    â”‚     â””â”€ Schema: order_id, customer_id, amount, order_date, _metadata
    â”‚
    â”œâ”€ STEP 7.2: Data Validation & Quality Checks
    â”‚  â”‚
    â”‚  â”œâ”€ Row count validation:
    â”‚  â”‚  â”œâ”€ Count: 1000 rows
    â”‚  â”‚  â””â”€ Status: PASS âœ…
    â”‚  â”‚
    â”‚  â”œâ”€ Null value checks:
    â”‚  â”‚  â”œâ”€ order_id: 0 nulls âœ…
    â”‚  â”‚  â”œâ”€ customer_id: 0 nulls âœ…
    â”‚  â”‚  â”œâ”€ amount: 0 nulls âœ…
    â”‚  â”‚  â””â”€ order_date: 0 nulls âœ…
    â”‚  â”‚
    â”‚  â”œâ”€ Data type validation:
    â”‚  â”‚  â”œâ”€ order_id: BIGINT âœ…
    â”‚  â”‚  â”œâ”€ customer_id: BIGINT âœ…
    â”‚  â”‚  â”œâ”€ amount: DECIMAL(10,2) âœ…
    â”‚  â”‚  â””â”€ order_date: DATE âœ…
    â”‚  â”‚
    â”‚  â”œâ”€ Business logic validation:
    â”‚  â”‚  â”œâ”€ amount > 0: 998 rows âœ…
    â”‚  â”‚  â”œâ”€ amount < 1000000: 1000 rows âœ…
    â”‚  â”‚  â”œâ”€ order_date >= 2025-01-01: 1000 rows âœ…
    â”‚  â”‚  â””â”€ Reject 2 invalid rows â†’ Error table
    â”‚  â”‚
    â”‚  â””â”€ Log metrics to PostgreSQL
    â”‚
    â”œâ”€ STEP 7.3: Transform to Standard Schema
    â”‚  â”‚
    â”‚  â”œâ”€ Column transformations:
    â”‚  â”‚  â”œâ”€ order_id: No change
    â”‚  â”‚  â”œâ”€ customer_id: No change
    â”‚  â”‚  â”œâ”€ amount: Round to 2 decimals
    â”‚  â”‚  â”œâ”€ order_date: Convert to DATE type
    â”‚  â”‚  â”œâ”€ processing_date: 2025-01-15
    â”‚  â”‚  â”œâ”€ ingest_timestamp: Extract from _metadata.ingest_timestamp
    â”‚  â”‚  â”œâ”€ data_version: Set to "v1"
    â”‚  â”‚  â””â”€ source_system: Set to "datasource-api"
    â”‚  â”‚
    â”‚  â”œâ”€ Add computed columns:
    â”‚  â”‚  â”œâ”€ year: Extract from order_date â†’ 2025
    â”‚  â”‚  â”œâ”€ month: Extract from order_date â†’ 01
    â”‚  â”‚  â”œâ”€ day: Extract from order_date â†’ 15
    â”‚  â”‚  â””â”€ updated_at: Current timestamp
    â”‚  â”‚
    â”‚  â””â”€ Final schema:
    â”‚     â”œâ”€ order_id (BIGINT)
    â”‚     â”œâ”€ customer_id (BIGINT)
    â”‚     â”œâ”€ amount (DECIMAL(10,2))
    â”‚     â”œâ”€ order_date (DATE)
    â”‚     â”œâ”€ processing_date (DATE)
    â”‚     â”œâ”€ ingest_timestamp (TIMESTAMP)
    â”‚     â”œâ”€ data_version (STRING)
    â”‚     â”œâ”€ source_system (STRING)
    â”‚     â”œâ”€ year (INT)
    â”‚     â”œâ”€ month (INT)
    â”‚     â”œâ”€ day (INT)
    â”‚     â””â”€ updated_at (TIMESTAMP)
    â”‚
    â”œâ”€ STEP 7.4: Register Iceberg Catalog (Lakekeeper)
    â”‚  â”‚
    â”‚  â”œâ”€ Initialize Iceberg Catalog:
    â”‚  â”‚  â”œâ”€ Type: REST Catalog (Lakekeeper)
    â”‚  â”‚  â”œâ”€ URI: http://lakekeeper:8080
    â”‚  â”‚  â””â”€ Warehouse: s3a://warehouse/bronze/
    â”‚  â”‚
    â”‚  â””â”€ Code:
    â”‚     spark.sql.catalog.iceberg = \
    â”‚     org.apache.iceberg.spark.SparkCatalog
    â”‚     spark.sql.catalog.iceberg.type = rest
    â”‚     spark.sql.catalog.iceberg.uri = \
    â”‚     http://lakekeeper:8080
    â”‚     spark.sql.catalog.iceberg.warehouse = \
    â”‚     s3a://warehouse/bronze/
    â”‚
    â”œâ”€ STEP 7.5: Write to Iceberg Table (Bronze Layer)
    â”‚  â”‚
    â”‚  â”œâ”€ Table namespace: bronze_warehouse
    â”‚  â”œâ”€ Table name: orders
    â”‚  â”œâ”€ Full path: bronze_warehouse.orders
    â”‚  â”‚
    â”‚  â”œâ”€ Create or merge to Iceberg table:
    â”‚  â”‚  df_bronze.writeTo("iceberg.bronze_warehouse.orders")
    â”‚  â”‚     .tableProperty("write.merge.mode", "copy-on-write")
    â”‚  â”‚     .tableProperty("format-version", "2")
    â”‚  â”‚     .mode("append")
    â”‚  â”‚     .partitionedBy("year", "month", "day")
    â”‚  â”‚     .option("write.parquet.compression-codec", "snappy")
    â”‚  â”‚     .option("iceberg.parquet.use-spark-writeSchema", "true")
    â”‚  â”‚     .saveAsTable()
    â”‚  â”‚
    â”‚  â”œâ”€ Write result:
    â”‚  â”‚  â”œâ”€ Rows written: 998
    â”‚  â”‚  â”œâ”€ Rows rejected: 2
    â”‚  â”‚  â”œâ”€ Duration: 25 seconds
    â”‚  â”‚  â””â”€ Output path: s3a://warehouse/bronze/orders/year=2025/month=01/day=15/
    â”‚  â”‚
    â”‚  â””â”€ Iceberg metadata updated:
    â”‚     â”œâ”€ Current snapshot ID: 4567890
    â”‚     â”œâ”€ Manifest files: v1, v2, v3
    â”‚     â”œâ”€ Partition data files: 3 files
    â”‚     â””â”€ Schema evolution: OK (backward compatible)
    â”‚
    â”œâ”€ STEP 7.6: Register Table in Lakekeeper
    â”‚  â”‚
    â”‚  â”œâ”€ Call Lakekeeper REST API:
    â”‚  â”‚  POST http://lakekeeper:8080/catalogs/bronze_warehouse/namespaces/default/tables
    â”‚  â”‚
    â”‚  â”œâ”€ Metadata:
    â”‚  â”‚  â”œâ”€ Table name: orders
    â”‚  â”‚  â”œâ”€ Location: s3a://warehouse/bronze/orders/
    â”‚  â”‚  â”œâ”€ Format: ICEBERG
    â”‚  â”‚  â”œâ”€ Partition columns: year, month, day
    â”‚  â”‚  â”œâ”€ Columns: (as defined above)
    â”‚  â”‚  â”œâ”€ Created at: 2025-01-15T14:30:50Z
    â”‚  â”‚  â”œâ”€ Modified at: 2025-01-15T14:30:50Z
    â”‚  â”‚  â””â”€ Table properties:
    â”‚  â”‚     â”œâ”€ source_system: datasource-api
    â”‚  â”‚     â”œâ”€ data_version: v1
    â”‚  â”‚     â”œâ”€ sla: 24h
    â”‚  â”‚     â””â”€ owner: data-platform-team
    â”‚  â”‚
    â”‚  â”œâ”€ Lakekeeper response: 201 Created
    â”‚  â”‚  â”œâ”€ Warehouse ID: bronze_warehouse_123
    â”‚  â”‚  â””â”€ Table ID: orders_456
    â”‚  â”‚
    â”‚  â””â”€ Metadata stored in Lakekeeper
    â”‚
    â”œâ”€ STEP 7.7: Log Job Metrics
    â”‚  â”‚
    â”‚  â”œâ”€ Write to PostgreSQL: job_metrics table
    â”‚  â”‚  â”œâ”€ job_id: bronze-dag-20250115-143000-001
    â”‚  â”‚  â”œâ”€ job_name: bronze_transform
    â”‚  â”‚  â”œâ”€ status: SUCCESS
    â”‚  â”‚  â”œâ”€ input_records: 1000
    â”‚  â”‚  â”œâ”€ output_records: 998
    â”‚  â”‚  â”œâ”€ rejected_records: 2
    â”‚  â”‚  â”œâ”€ duration_seconds: 45
    â”‚  â”‚  â”œâ”€ start_time: 2025-01-15T14:30:20Z
    â”‚  â”‚  â”œâ”€ end_time: 2025-01-15T14:31:05Z
    â”‚  â”‚  â”œâ”€ spark_application_id: application_1705325400000_0001
    â”‚  â”‚  â”œâ”€ table_name: bronze_warehouse.orders
    â”‚  â”‚  â”œâ”€ output_path: s3a://warehouse/bronze/orders/year=2025/month=01/day=15/
    â”‚  â”‚  â””â”€ created_at: 2025-01-15T14:31:05Z
    â”‚  â”‚
    â”‚  â””â”€ Write to PostgreSQL: data_quality_metrics table
    â”‚     â”œâ”€ job_id: bronze-dag-20250115-143000-001
    â”‚     â”œâ”€ check_name: null_check_order_id
    â”‚     â”œâ”€ check_status: PASS
    â”‚     â”œâ”€ check_value: 0 nulls
    â”‚     â””â”€ created_at: 2025-01-15T14:31:05Z
    â”‚
    â””â”€ STEP 7.8: Spark Job Completes
       â”‚
       â”œâ”€ Status: SUCCESS âœ…
       â”œâ”€ Exit code: 0
       â”œâ”€ Total duration: 45 seconds
       â””â”€ Driver logs available in kubectl

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: AIRFLOW TASK 4 - MONITOR SPARK JOB COMPLETION         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow Task 4: wait_spark_completion
    â”‚
    â”œâ”€ Monitor SparkApplication status via Kubernetes:
    â”‚  â”œâ”€ kubectl get sparkapplication bronze-dag-20250115-143000-001
    â”‚  â”œâ”€ Poll every 10 seconds
    â”‚  â””â”€ Timeout: 30 minutes
    â”‚
    â”œâ”€ Check status conditions:
    â”‚  â”œâ”€ Phase: SUCCEEDED âœ…
    â”‚  â”œâ”€ Conditions:
    â”‚  â”‚  â”œâ”€ Type: Submitted âœ…
    â”‚  â”‚  â”œâ”€ Type: Running âœ…
    â”‚  â”‚  â”œâ”€ Type: Succeeded âœ…
    â”‚  â”‚  â””â”€ Message: Application completed successfully
    â”‚  â”‚
    â”‚  â””â”€ Driver pod logs:
    â”‚     â”œâ”€ Timestamp 14:30:50: Spark context started
    â”‚     â”œâ”€ Timestamp 14:31:00: Reading from S3
    â”‚     â”œâ”€ Timestamp 14:31:15: Data validation completed
    â”‚     â”œâ”€ Timestamp 14:31:30: Writing to Iceberg
    â”‚     â””â”€ Timestamp 14:31:05: Job completed successfully
    â”‚
    â”œâ”€ Extract job metrics:
    â”‚  â”œâ”€ Query PostgreSQL job_metrics table:
    â”‚  â”‚  â””â”€ WHERE job_id = 'bronze-dag-20250115-143000-001'
    â”‚  â”‚
    â”‚  â””â”€ Retrieve:
    â”‚     â”œâ”€ output_records: 998
    â”‚     â”œâ”€ rejected_records: 2
    â”‚     â”œâ”€ duration_seconds: 45
    â”‚     â””â”€ table_name: bronze_warehouse.orders
    â”‚
    â”œâ”€ Verify success criteria:
    â”‚  â”œâ”€ Spark job status: SUCCESS âœ…
    â”‚  â”œâ”€ Output records > 0: 998 > 0 âœ…
    â”‚  â”œâ”€ Iceberg table exists: âœ…
    â”‚  â””â”€ Lakekeeper registration: âœ…
    â”‚
    â””â”€ Task Status: SUCCESS âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: AIRFLOW TASK 5 - PUBLISH SUCCESS & CLEANUP            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Airflow Task 5: publish_success_metrics
    â”‚
    â”œâ”€ Publish success message to Kafka:
    â”‚  â””â”€ Topic: bronze-layer-complete
    â”‚     Message:
    â”‚     {
    â”‚       "upload_id": "upload-20250115-001",
    â”‚       "workflow_id": "data-ingestion-20250115-143000-001",
    â”‚       "status": "SUCCESS",
    â”‚       "table_name": "bronze_warehouse.orders",
    â”‚       "output_records": 998,
    â”‚       "rejected_records": 2,
    â”‚       "processing_date": "2025-01-15",
    â”‚       "total_duration_seconds": 75,
    â”‚       "completed_at": "2025-01-15T14:31:15Z"
    â”‚     }
    â”‚
    â”œâ”€ Clean up temporary files:
    â”‚  â”œâ”€ Delete: /tmp/spark-bronze-job-*.yaml
    â”‚  â””â”€ Status: Cleaned
    â”‚
    â”œâ”€ Log overall metrics:
    â”‚  â””â”€ PostgreSQL: workflow_execution table
    â”‚     â”œâ”€ workflow_id: data-ingestion-20250115-143000-001
    â”‚     â”œâ”€ status: SUCCESS
    â”‚     â”œâ”€ start_time: 2025-01-15T14:30:00Z
    â”‚     â”œâ”€ end_time: 2025-01-15T14:31:15Z
    â”‚     â”œâ”€ total_duration_seconds: 75
    â”‚     â”œâ”€ tasks_completed: 5
    â”‚     â”œâ”€ tasks_failed: 0
    â”‚     â””â”€ created_at: 2025-01-15T14:31:15Z
    â”‚
    â””â”€ Task Status: SUCCESS âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPLETE WORKFLOW SUMMARY                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline:
  14:30:00 - File uploaded to Datasource API
  14:30:01 - Message published to Kafka (file-uploaded)
  14:30:02 - Airflow Sensor detects event
  14:30:03 - Airflow DAG triggered
  14:30:05 - Task 1: trigger_nifi_flow (2 seconds)
  14:30:07 - NiFi ingest starts
  14:30:20 - Raw file written to MinIO
  14:30:21 - Message published to Kafka (raw-data-ready)
  14:30:22 - Task 2: wait_nifi_completion (15 seconds)
  14:30:23 - Task 3: submit_spark_job (1 second)
  14:30:24 - Spark job submitted to K8s
  14:30:25 - Spark driver & executors starting
  14:30:35 - Spark job executing transformation
  14:31:05 - Spark job completed
  14:31:10 - Task 4: wait_spark_completion (35 seconds)
  14:31:15 - Task 5: publish_success_metrics (5 seconds)
  14:31:15 - WORKFLOW COMPLETE âœ…

Total Duration: 1 minute 15 seconds

Data Journey:
  User's CSV â†’ Datasource API â†’ MinIO (uploads/) â†’
  NiFi Ingestion â†’ MinIO (raw/) â†’ Spark Transformation â†’
  Iceberg Bronze Warehouse â†’ Lakekeeper Metadata Catalog

Success Metrics:
  âœ… Input records: 1000
  âœ… Output records: 998
  âœ… Rejected records: 2
  âœ… Iceberg table: bronze_warehouse.orders
  âœ… Partitions: year=2025, month=01, day=15
  âœ… Data version: v1
  âœ… Source system: datasource-api
```

---

## 3. Implementation Details

### 3.1 Project Folder Structure

```
data-lakehouse/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ SETUP.md
â”‚   â”œâ”€â”€ FLOW.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ k8s/
â”‚   â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”‚   â”œâ”€â”€ storage-class.yaml
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka-topics.yaml
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â”œâ”€â”€ install_kafka.sh
â”‚   â”‚   â”‚       â””â”€â”€ create_topics.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ nifi/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ nifi-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ processor-groups.json
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â”œâ”€â”€ install_nifi.sh
â”‚   â”‚   â”‚       â””â”€â”€ configure_nifi.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ spark-operator/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ spark-operator-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ spark-rbac.yaml
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â””â”€â”€ install_spark_operator.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ airflow-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ airflow-rbac.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ airflow-configs.yaml
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â”œâ”€â”€ install_airflow.sh
â”‚   â”‚   â”‚       â””â”€â”€ configure_airflow.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ minio/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ minio-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ minio-init-bucket.yaml
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â”œâ”€â”€ install_minio.sh
â”‚   â”‚   â”‚       â””â”€â”€ create_buckets.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ postgresql/
â”‚   â”‚   â”‚   â”œâ”€â”€ helm-values.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ postgresql-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ database-init.sql
â”‚   â”‚   â”‚   â””â”€â”€ scripts/
â”‚   â”‚   â”‚       â””â”€â”€ install_postgresql.sh
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ lakekeeper/
â”‚   â”‚       â”œâ”€â”€ helm-values.yaml
â”‚   â”‚       â”œâ”€â”€ lakekeeper-deployment.yaml
â”‚   â”‚       â””â”€â”€ scripts/
â”‚   â”‚           â””â”€â”€ install_lakekeeper.sh
â”‚   â”‚
â”‚   â”œâ”€â”€ docker-compose/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml (Local dev)
â”‚   â”‚   â””â”€â”€ .env.example
â”‚   â”‚
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ setup_k8s_cluster.sh
â”‚       â”œâ”€â”€ setup_namespaces.sh
â”‚       â”œâ”€â”€ setup_secrets.sh
â”‚       â”œâ”€â”€ install_all_components.sh
â”‚       â””â”€â”€ cleanup_all.sh
â”‚
â”œâ”€â”€ airflow-dags/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py          # MAIN DAG
â”‚   â”‚   â””â”€â”€ monitoring_dag.py
â”‚   â”‚
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sensors/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ kafka_sensor.py         # Custom Kafka sensor
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ nifi_operator.py        # Trigger NiFi flow
â”‚   â”‚   â”‚   â””â”€â”€ spark_operator.py       # Submit Spark job
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ nifi_hook.py            # NiFi API connection
â”‚   â”‚       â”œâ”€â”€ kafka_hook.py           # Kafka connection
â”‚   â”‚       â””â”€â”€ k8s_hook.py             # Kubernetes connection
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â”‚   â””â”€â”€ logging.conf
â”‚   â”‚
â”‚   â””â”€â”€ docker/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ entrypoint.sh
â”‚
â”œâ”€â”€ spark-jobs/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ bronze-layer/
â”‚   â”‚   â”œâ”€â”€ bronze_transform.py         # Main Spark job
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”‚       â”œâ”€â”€ test_bronze_transform.py
â”‚   â”‚       â””â”€â”€ test_data.py
â”‚   â”‚
â”‚   â”œâ”€â”€ spark-configs/
â”‚   â”‚   â”œâ”€â”€ spark-defaults.conf
â”‚   â”‚   â””â”€â”€ log4j.properties
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ iceberg_utils.py            # Iceberg operations
â”‚       â”œâ”€â”€ data_quality.py             # DQ checks
â”‚       â”œâ”€â”€ s3_utils.py                 # MinIO operations
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ datasource-api/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ upload.py                  # File upload endpoint
â”‚   â”‚   â”œâ”€â”€ health.py                  # Health check
â”‚   â”‚   â””â”€â”€ metadata.py                # Metadata retrieval
â”‚   â”‚
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ upload.py                  # Pydantic schemas
â”‚
â”œâ”€â”€ kubernetes-manifests/
â”‚   â”œâ”€â”€ spark-job-templates/
â”‚   â”‚   â”œâ”€â”€ bronze-transform.yaml       # SparkApplication template
â”‚   â”‚   â”œâ”€â”€ silver-transform.yaml
â”‚   â”‚   â””â”€â”€ gold-transform.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ configmaps/
â”‚   â”‚   â”œâ”€â”€ spark-config.yaml
â”‚   â”‚   â””â”€â”€ airflow-config.yaml
â”‚   â”‚
â”‚   â””â”€â”€ secrets/
â”‚       â”œâ”€â”€ minio-credentials.yaml
â”‚       â””â”€â”€ database-credentials.yaml
â”‚
â””â”€â”€ monitoring/
    â”œâ”€â”€ prometheus/
    â”‚   â”œâ”€â”€ prometheus-config.yaml
    â”‚   â””â”€â”€ prometheus-deployment.yaml
    â”‚
    â””â”€â”€ grafana/
        â”œâ”€â”€ grafana-deployment.yaml
        â””â”€â”€ dashboards/
            â”œâ”€â”€ airflow-dashboard.json
            â”œâ”€â”€ spark-dashboard.json
            â””â”€â”€ pipeline-dashboard.json
```

### 3.2 Airflow DAG Implementation

```python
# airflow-dags/dags/data_ingestion.py

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.kafka.sensors.kafka import KafkaConsumerSensor
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.utils.decorators import apply_defaults
import json
import subprocess
import time
from plugins.operators.nifi_operator import NiFiOperator
from plugins.operators.spark_operator import SparkOperator
import logging

logger = logging.getLogger(__name__)

# DAG Configuration
default_args = {
    'owner': 'data-platform-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email': ['data-team@company.com']
}

dag = DAG(
    'data_ingestion',
    default_args=default_args,
    description='Ingest raw data â†’ transform to Bronze Iceberg layer',
    schedule_interval=None,  # Trigger manually or from Kafka
    catchup=False,
    tags=['data-ingestion', 'bronze-layer']
)

# ============================================================================
# TASK 1: Listen to Kafka & Trigger DAG
# ============================================================================

# This sensor runs on a schedule and listens to Kafka topic
# When a message arrives, it triggers this DAG run
kafka_sensor = KafkaConsumerSensor(
    task_id='listen_kafka_file_uploaded',
    topics=['file-uploaded'],
    bootstrap_servers=['kafka:9092'],
    group_id='airflow-data-ingestion',
    api_version=(0, 10, 1),
    consumer_timeout_ms=5000,
    mode='once',
    poke_interval=10,
    timeout=600,
    dag=dag,
)

# ============================================================================
# TASK 2: Trigger NiFi Data Ingestion Flow
# ============================================================================

def trigger_nifi_flow(**context):
    """
    Trigger NiFi to ingest raw file to MinIO
    
    Input: Kafka message from task 1
    Output: raw file in MinIO (s3a://raw/)
    """
    
    # Get Kafka message from sensor
    kafka_message = context['task_instance'].xcom_pull(
        task_ids='listen_kafka_file_uploaded'
    )
    
    logger.info(f"Received Kafka message: {kafka_message}")
    
    # Parse message
    message_data = json.loads(kafka_message)
    filename = message_data['filename']
    upload_path = message_data['upload_path']
    upload_id = message_data['upload_id']
    
    logger.info(f"Triggering NiFi for file: {filename}")
    
    # Call NiFi REST API to start process group
    nifi_api_url = "http://nifi:8080/nifi-api"
    process_group_id = "data-ingestion-pg-xyz"  # Get from NiFi UI
    
    # Get process group details
    response = subprocess.run([
        'curl', '-s',
        f'{nifi_api_url}/process-groups/{process_group_id}'
    ], capture_output=True, text=True)
    
    pg_state = json.loads(response.stdout)
    
    # Start process group
    start_payload = {
        "id": process_group_id,
        "state": "RUNNING"
    }
    
    response = subprocess.run([
        'curl', '-X', 'PUT',
        '-H', 'Content-Type: application/json',
        '-d', json.dumps(start_payload),
        f'{nifi_api_url}/process-groups/{process_group_id}'
    ], capture_output=True, text=True)
    
    logger.info(f"NiFi response: {response.stdout}")
    
    # Save to XCom for next task
    context['task_instance'].xcom_push(
        key='nifi_flow_id',
        value=process_group_id
    )
    
    context['task_instance'].xcom_push(
        key='filename',
        value=filename
    )
    
    context['task_instance'].xcom_push(
        key='upload_id',
        value=upload_id
    )
    
    context['task_instance'].xcom_push(
        key='upload_path',
        value=upload_path
    )
    
    logger.info(f"âœ… NiFi flow triggered successfully")

trigger_nifi = PythonOperator(
    task_id='trigger_nifi_flow',
    python_callable=trigger_nifi_flow,
    provide_context=True,
    dag=dag
)

# ============================================================================
# TASK 3: Wait for NiFi Completion (via Kafka)
# ============================================================================

def wait_nifi_completion(**context):
    """
    Wait for NiFi to complete ingestion
    Monitor Kafka topic: raw-data-ready
    """
    
    upload_id = context['task_instance'].xcom_pull(
        task_ids='trigger_nifi_flow',
        key='upload_id'
    )
    
    logger.info(f"Waiting for NiFi completion (upload_id: {upload_id})")
    
    # Listen to Kafka topic: raw-data-ready
    bootstrap_servers = ['kafka:9092']
    topic = 'raw-data-ready'
    group_id = f'airflow-nifi-monitor-{upload_id}'
    
    from confluent_kafka import Consumer, KafkaError
    
    config = {
        'bootstrap.servers': ','.join(bootstrap_servers),
        'group.id': group_id,
        'auto.offset.reset': 'latest',
        'enable.auto.commit': True,
    }
    
    consumer = Consumer(config)
    consumer.subscribe([topic])
    
    timeout_seconds = 300  # 5 minutes
    start_time = time.time()
    raw_file_path = None
    
    try:
        while True:
            msg = consumer.poll(timeout=1)
            
            if msg is None:
                # No message, check timeout
                if time.time() - start_time > timeout_seconds:
                    raise Exception(f"Timeout waiting for NiFi completion (upload_id: {upload_id})")
                continue
            
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                else:
                    raise KafkaError(msg.error())
            
            # Parse message
            message_data = json.loads(msg.value().decode('utf-8'))
            
            # Check if this is our message
            if message_data.get('upload_id') == upload_id:
                logger.info(f"âœ… NiFi completed: {message_data}")
                raw_file_path = message_data['file_path']
                record_count = message_data['record_count']
                
                # Save to XCom
                context['task_instance'].xcom_push(
                    key='raw_file_path',
                    value=raw_file_path
                )
                
                context['task_instance'].xcom_push(
                    key='record_count',
                    value=record_count
                )
                
                break
    
    finally:
        consumer.close()
    
    logger.info(f"Raw file ready: {raw_file_path}")

wait_nifi = PythonOperator(
    task_id='wait_nifi_completion',
    python_callable=wait_nifi_completion,
    provide_context=True,
    dag=dag,
    timeout=600  # 10 minutes
)

# ============================================================================
# TASK 4: Submit Spark Job (Raw â†’ Bronze)
# ============================================================================

def submit_spark_job(**context):
    """
    Submit Spark job to K8s via kubectl apply
    Transform: raw â†’ bronze (Iceberg)
    """
    
    # Get parameters from previous tasks
    filename = context['task_instance'].xcom_pull(
        task_ids='trigger_nifi_flow',
        key='filename'
    )
    
    upload_id = context['task_instance'].xcom_pull(
        task_ids='trigger_nifi_flow',
        key='upload_id'
    )
    
    raw_file_path = context['task_instance'].xcom_pull(
        task_ids='wait_nifi_completion',
        key='raw_file_path'
    )
    
    dag_run_id = context['dag_run'].run_id
    job_id = f"bronze-{dag_run_id}"
    
    logger.info(f"Submitting Spark job: {job_id}")
    logger.info(f"Input: {raw_file_path}")
    
    # Load SparkApplication template
    template_path = "/opt/airflow/kubernetes-manifests/spark-job-templates/bronze-transform.yaml"
    
    with open(template_path, 'r') as f:
        template_content = f.read()
    
    # Render template with parameters
    from jinja2 import Template
    
    template = Template(template_content)
    
    manifest = template.render(
        job_id=job_id,
        input_path=raw_file_path,
        output_warehouse="bronze_warehouse",
        table_name="orders",  # Can be derived from filename
        processing_date=datetime.now().strftime("%Y-%m-%d"),
        lakekeeper_uri="http://lakekeeper:8080",
        minio_endpoint="http://minio:9000",
        minio_access_key="{{ env.MINIO_ACCESS_KEY }}",
        minio_secret_key="{{ env.MINIO_SECRET_KEY }}",
        driver_cores=2,
        driver_memory="2Gi",
        executor_cores=2,
        executor_instances=3,
        executor_memory="4Gi"
    )
    
    # Write manifest to temp file
    manifest_file = f"/tmp/spark-{job_id}.yaml"
    with open(manifest_file, 'w') as f:
        f.write(manifest)
    
    logger.info(f"Manifest written to: {manifest_file}")
    
    # Submit to Kubernetes
    cmd = [
        'kubectl', 'apply',
        '-f', manifest_file,
        '-n', 'data-platform'
    ]
    
    logger.info(f"Executing: {' '.join(cmd)}")
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        raise Exception(f"kubectl apply failed: {result.stderr}")
    
    logger.info(f"âœ… SparkApplication created: {job_id}")
    logger.info(f"Response: {result.stdout}")
    
    # Save job_id to XCom for next task
    context['task_instance'].xcom_push(
        key='spark_job_id',
        value=job_id
    )
    
    # Clean up temp file
    import os
    os.remove(manifest_file)

submit_spark = PythonOperator(
    task_id='submit_spark_job',
    python_callable=submit_spark_job,
    provide_context=True,
    dag=dag
)

# ============================================================================
# TASK 5: Wait for Spark Job Completion
# ============================================================================

def wait_spark_completion(**context):
    """
    Monitor Spark job status via kubectl
    Poll SparkApplication resource until completion
    """
    
    job_id = context['task_instance'].xcom_pull(
        task_ids='submit_spark_job',
        key='spark_job_id'
    )
    
    logger.info(f"Waiting for Spark job: {job_id}")
    
    namespace = "data-platform"
    timeout_seconds = 1800  # 30 minutes
    start_time = time.time()
    poll_interval = 10
    
    while True:
        # Get SparkApplication status
        cmd = [
            'kubectl', 'get', 'sparkapplication', job_id,
            '-n', namespace,
            '-o', 'json'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            raise Exception(f"kubectl get failed: {result.stderr}")
        
        spark_app = json.loads(result.stdout)
        
        status = spark_app['status'].get('phase', 'UNKNOWN')
        conditions = spark_app['status'].get('conditions', [])
        
        logger.info(f"Spark job status: {status}")
        
        # Check completion
        if status == 'SUCCEEDED':
            logger.info(f"âœ… Spark job SUCCEEDED: {job_id}")
            
            # Extract metrics
            driver_pod = spark_app['status'].get('driverInfo', {})
            
            context['task_instance'].xcom_push(
                key='spark_status',
                value='SUCCESS'
            )
            
            context['task_instance'].xcom_push(
                key='spark_job_id',
                value=job_id
            )
            
            break
        
        elif status == 'FAILED':
            logger.error(f"âŒ Spark job FAILED: {job_id}")
            
            # Get logs
            driver_pod_name = spark_app['status'].get('driverInfo', {}).get('podName')
            if driver_pod_name:
                log_cmd = [
                    'kubectl', 'logs', driver_pod_name,
                    '-n', namespace
                ]
                log_result = subprocess.run(log_cmd, capture_output=True, text=True)
                logger.error(f"Driver logs:\n{log_result.stdout}")
            
            raise Exception(f"Spark job FAILED: {job_id}")
        
        elif status == 'RUNNING':
            logger.info(f"Spark job still running, will check again in {poll_interval}s")
        
        # Check timeout
        if time.time() - start_time > timeout_seconds:
            raise Exception(f"Timeout waiting for Spark job: {job_id}")
        
        # Wait before next poll
        time.sleep(poll_interval)

wait_spark = PythonOperator(
    task_id='wait_spark_completion',
    python_callable=wait_spark_completion,
    provide_context=True,
    dag=dag,
    timeout=2100  # 35 minutes (timeout_seconds + buffer)
)

# ============================================================================
# TASK 6: Publish Success & Cleanup
# ============================================================================

def publish_success(**context):
    """
    Publish success message to Kafka
    Log workflow metrics to PostgreSQL
    """
    
    upload_id = context['task_instance'].xcom_pull(
        task_ids='trigger_nifi_flow',
        key='upload_id'
    )
    
    filename = context['task_instance'].xcom_pull(
        task_ids='trigger_nifi_flow',
        key='filename'
    )
    
    dag_run_id = context['dag_run'].run_id
    job_id = f"bronze-{dag_run_id}"
    
    # Get metrics from PostgreSQL
    import psycopg2
    
    conn = psycopg2.connect(
        host="postgresql",
        database="airflow",
        user="airflow",
        password="airflow"
    )
    
    cur = conn.cursor()
    
    # Query job metrics
    cur.execute("""
        SELECT output_records, rejected_records, duration_seconds
        FROM public.job_metrics
        WHERE job_id = %s
        ORDER BY created_at DESC
        LIMIT 1
    """, (job_id,))
    
    row = cur.fetchone()
    
    if row:
        output_records, rejected_records, duration_seconds = row
    else:
        output_records = 0
        rejected_records = 0
        duration_seconds = 0
    
    cur.close()
    conn.close()
    
    # Publish to Kafka
    from confluent_kafka import Producer
    
    config = {
        'bootstrap.servers': 'kafka:9092',
    }
    
    producer = Producer(config)
    
    message = {
        "upload_id": upload_id,
        "filename": filename,
        "job_id": job_id,
        "status": "SUCCESS",
        "output_records": output_records,
        "rejected_records": rejected_records,
        "duration_seconds": duration_seconds,
        "completed_at": datetime.now().isoformat()
    }
    
    producer.produce(
        'bronze-layer-complete',
        json.dumps(message).encode('utf-8')
    )
    
    producer.flush()
    
    logger.info(f"âœ… Success message published to Kafka")
    logger.info(f"Message: {message}")

publish_success_task = PythonOperator(
    task_id='publish_success',
    python_callable=publish_success,
    provide_context=True,
    dag=dag
)

# ============================================================================
# DAG Dependency Graph
# ============================================================================

kafka_sensor >> trigger_nifi >> wait_nifi >> submit_spark >> wait_spark >> publish_success_task
```

### 3.3 SparkApplication Manifest Template

```yaml
# kubernetes-manifests/spark-job-templates/bronze-transform.yaml

apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: {{ job_id }}
  namespace: data-platform
  labels:
    app: data-lakehouse
    layer: bronze
    job-id: {{ job_id }}
  annotations:
    description: "Transform raw â†’ bronze (Iceberg)"

spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: spark:3.4.0-python3
  imagePullPolicy: IfNotPresent
  
  # Main application file (stored in MinIO)
  mainApplicationFile: "s3a://spark-jobs/bronze_transform.py"
  
  # Arguments to Spark job
  arguments:
    - "--input-path"
    - "{{ input_path }}"
    - "--output-warehouse"
    - "{{ output_warehouse }}"
    - "--table-name"
    - "{{ table_name }}"
    - "--processing-date"
    - "{{ processing_date }}"
    - "--lakekeeper-uri"
    - "{{ lakekeeper_uri }}"
    - "--minio-endpoint"
    - "{{ minio_endpoint }}"
    - "--minio-access-key"
    - "{{ minio_access_key }}"
    - "--minio-secret-key"
    - "{{ minio_secret_key }}"
  
  sparkVersion: "3.4.0"
  
  restartPolicy:
    type: Never
  
  # Driver configuration
  driver:
    cores: {{ driver_cores }}
    memory: {{ driver_memory }}
    memoryOverhead: 256m
    serviceAccount: spark
    volumeMounts:
      - name: spark-jars
        mountPath: /opt/spark/jars
  
  # Executor configuration
  executor:
    cores: {{ executor_cores }}
    instances: {{ executor_instances }}
    memory: {{ executor_memory }}
    memoryOverhead: 512m
    volumeMounts:
      - name: spark-jars
        mountPath: /opt/spark/jars
  
  # JVM configurations
  sparkConf:
    # Iceberg Catalog Configuration
    "spark.sql.catalog.iceberg": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.iceberg.type": "rest"
    "spark.sql.catalog.iceberg.uri": "{{ lakekeeper_uri }}"
    "spark.sql.catalog.iceberg.warehouse": "s3a://warehouse/bronze/"
    "spark.sql.catalog.iceberg.s3.endpoint": "{{ minio_endpoint }}"
    "spark.sql.catalog.iceberg.s3.access-key-id": "{{ minio_access_key }}"
    "spark.sql.catalog.iceberg.s3.secret-access-key": "{{ minio_secret_key }}"
    
    # MinIO S3 Configuration
    "spark.hadoop.fs.s3a.endpoint": "{{ minio_endpoint }}"
    "spark.hadoop.fs.s3a.access.key": "{{ minio_access_key }}"
    "spark.hadoop.fs.s3a.secret.key": "{{ minio_secret_key }}"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    
    # Performance tuning
    "spark.sql.shuffle.partitions": "200"
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
    "spark.sql.adaptive.skewJoin.enabled": "true"
    
    # Data source format
    "spark.sql.defaultCatalog": "iceberg"
    "spark.sql.parquet.compression.codec": "snappy"
    
    # Logging
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://logs/spark/"
  
  # Environment variables
  envVars:
    HADOOP_OPTIONAL_TOOLS: "hadoop-aws"
    AWS_S3_ENDPOINT: "{{ minio_endpoint }}"
    AWS_ACCESS_KEY_ID: "{{ minio_access_key }}"
    AWS_SECRET_ACCESS_KEY: "{{ minio_secret_key }}"
  
  # Volumes
  volumes:
    - name: spark-jars
      emptyDir: {}
  
  # Monitoring
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
    prometheus:
      jmxExporterConfig: |
        ---
        rules:
          - pattern: ".*"
```

### 3.4 Spark Job Implementation

```python
# spark-jobs/bronze-layer/bronze_transform.py

import argparse
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
import json
import psycopg2

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments"""
    
    parser = argparse.ArgumentParser(description='Transform raw â†’ bronze (Iceberg)')
    
    parser.add_argument('--input-path', required=True, help='Input path in MinIO')
    parser.add_argument('--output-warehouse', required=True, help='Iceberg warehouse name')
    parser.add_argument('--table-name', required=True, help='Target table name')
    parser.add_argument('--processing-date', required=True, help='Processing date (YYYY-MM-DD)')
    parser.add_argument('--lakekeeper-uri', required=True, help='Lakekeeper REST URI')
    parser.add_argument('--minio-endpoint', required=True, help='MinIO S3 endpoint')
    parser.add_argument('--minio-access-key', required=True, help='MinIO access key')
    parser.add_argument('--minio-secret-key', required=True, help='MinIO secret key')
    
    return parser.parse_args()

def init_spark_session(args):
    """Initialize Spark session with Iceberg & MinIO configuration"""
    
    spark = SparkSession.builder \
        .appName("bronze-transform") \
        .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.iceberg.type", "rest") \
        .config("spark.sql.catalog.iceberg.uri", args.lakekeeper_uri) \
        .config("spark.sql.catalog.iceberg.warehouse", "s3a://warehouse/bronze/") \
        .config("spark.hadoop.fs.s3a.endpoint", args.minio_endpoint) \
        .config("spark.hadoop.fs.s3a.access.key", args.minio_access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", args.minio_secret_key) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .getOrCreate()
    
    logger.info("âœ… Spark session initialized")
    return spark

def read_raw_data(spark, input_path):
    """Read raw Parquet file from MinIO"""
    
    logger.info(f"Reading raw data from: {input_path}")
    
    df = spark.read \
        .format("parquet") \
        .load(input_path)
    
    logger.info(f"âœ… Loaded {df.count()} records")
    logger.info(f"Schema: {df.schema}")
    
    return df

def validate_data_quality(spark, df, job_id, table_name):
    """Validate data quality and log results"""
    
    logger.info("Starting data quality validation...")
    
    input_records = df.count()
    logger.info(f"Input records: {input_records}")
    
    # Row count validation
    if input_records == 0:
        raise Exception("Input data is empty!")
    
    # Null value checks
    null_checks = {}
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        null_checks[column] = null_count
        logger.info(f"Null check - {column}: {null_count}")
    
    # Data type validation
    logger.info(f"Data types: {df.dtypes}")
    
    # Business logic validation
    # Example: filter orders with positive amount
    valid_df = df.filter(col("amount") > 0)
    valid_records = valid_df.count()
    rejected_records = input_records - valid_records
    
    logger.info(f"Valid records: {valid_records}")
    logger.info(f"Rejected records: {rejected_records}")
    
    # Log to PostgreSQL
    log_metrics_to_postgres(
        job_id=job_id,
        table_name=table_name,
        input_records=input_records,
        output_records=valid_records,
        rejected_records=rejected_records,
        checks=null_checks
    )
    
    return valid_df, valid_records, rejected_records

def transform_schema(df, processing_date):
    """Transform to standard Bronze schema"""
    
    logger.info("Transforming schema...")
    
    df_bronze = df \
        .withColumn("processing_date", lit(processing_date).cast(DateType())) \
        .withColumn("ingest_timestamp", current_timestamp()) \
        .withColumn("data_version", lit("v1")) \
        .withColumn("source_system", lit("datasource-api")) \
        .withColumn("year", year(col("order_date"))) \
        .withColumn("month", month(col("order_date"))) \
        .withColumn("day", dayofmonth(col("order_date"))) \
        .withColumn("updated_at", current_timestamp())
    
    # Select and order columns
    bronze_schema = [
        "order_id",
        "customer_id",
        "amount",
        "order_date",
        "processing_date",
        "ingest_timestamp",
        "data_version",
        "source_system",
        "year",
        "month",
        "day",
        "updated_at"
    ]
    
    df_bronze = df_bronze.select(bronze_schema)
    
    logger.info(f"âœ… Schema transformed")
    logger.info(f"New schema: {df_bronze.schema}")
    
    return df_bronze

def write_to_iceberg(spark, df, output_warehouse, table_name):
    """Write DataFrame to Iceberg table"""
    
    logger.info(f"Writing to Iceberg table: {output_warehouse}.{table_name}")
    
    full_table_name = f"iceberg.{output_warehouse}.{table_name}"
    
    # Write to Iceberg
    df.writeTo(full_table_name) \
        .tableProperty("write.merge.mode", "copy-on-write") \
        .tableProperty("format-version", "2") \
        .tableProperty("commit.retry.num-retries", "3") \
        .mode("append") \
        .partitionedBy("year", "month", "day") \
        .option("write.parquet.compression-codec", "snappy") \
        .option("iceberg.parquet.use-spark-writeSchema", "true") \
        .saveAsTable()
    
    logger.info(f"âœ… Data written to Iceberg")
    
    # Verify table
    df_check = spark.sql(f"SELECT COUNT(*) as count FROM {full_table_name}")
    row_count = df_check.collect()[0]['count']
    logger.info(f"âœ… Iceberg table row count: {row_count}")
    
    return row_count

def log_metrics_to_postgres(**metrics):
    """Log job execution metrics to PostgreSQL"""
    
    try:
        conn = psycopg2.connect(
            host="postgresql",
            database="airflow",
            user="airflow",
            password="airflow"
        )
        
        cur = conn.cursor()
        
        # Insert job metrics
        cur.execute("""
            INSERT INTO public.job_metrics 
            (job_id, table_name, input_records, output_records, rejected_records, status, duration_seconds, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            metrics['job_id'],
            metrics['table_name'],
            metrics['input_records'],
            metrics['output_records'],
            metrics['rejected_records'],
            'SUCCESS',
            0,
            datetime.now()
        ))
        
        # Insert data quality checks
        for column, null_count in metrics['checks'].items():
            cur.execute("""
                INSERT INTO public.data_quality_metrics
                (job_id, check_name, check_status, check_value, created_at)
                VALUES (%s, %s, %s, %s, %s)
            """, (
                metrics['job_id'],
                f"null_check_{column}",
                'PASS' if null_count == 0 else 'FAIL',
                str(null_count),
                datetime.now()
            ))
        
        conn.commit()
        cur.close()
        conn.close()
        
        logger.info("âœ… Metrics logged to PostgreSQL")
    
    except Exception as e:
        logger.error(f"Error logging metrics: {e}")

def main():
    """Main execution"""
    
    args = parse_args()
    
    logger.info("=" * 80)
    logger.info(f"BRONZE LAYER TRANSFORMATION")
    logger.info("=" * 80)
    logger.info(f"Input path: {args.input_path}")
    logger.info(f"Output warehouse: {args.output_warehouse}")
    logger.info(f"Table name: {args.table_name}")
    logger.info(f"Processing date: {args.processing_date}")
    
    try:
        # Initialize Spark
        spark = init_spark_session(args)
        
        # Read raw data
        df_raw = read_raw_data(spark, args.input_path)
        
        # Validate data quality
        job_id = spark.sparkContext.applicationId  # Get Spark app ID
        df_valid, output_records, rejected_records = validate_data_quality(
            spark, df_raw, job_id, args.table_name
        )
        
        # Transform schema
        df_bronze = transform_schema(df_valid, args.processing_date)
        
        # Write to Iceberg
        iceberg_row_count = write_to_iceberg(
            spark, df_bronze, args.output_warehouse, args.table_name
        )
        
        logger.info("=" * 80)
        logger.info(f"âœ… BRONZE LAYER TRANSFORMATION COMPLETED SUCCESSFULLY")
        logger.info(f"Input records: {df_raw.count()}")
        logger.info(f"Output records: {output_records}")
        logger.info(f"Rejected records: {rejected_records}")
        logger.info(f"Iceberg table: {args.output_warehouse}.{args.table_name}")
        logger.info("=" * 80)
    
    except Exception as e:
        logger.error(f"âŒ Error in bronze transformation: {e}", exc_info=True)
        raise
    
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
```

---

## 4. Tech Stack Summary

| Component | Technology | Purpose | Status |
|-----------|-----------|---------|--------|
| **Message Bus** | Apache Kafka | Orchestrate workflow via events | âœ… Included |
| **Orchestrator** | Apache Airflow | Trigger workflows, monitor jobs | âœ… Included |
| **Data Ingestion** | Apache NiFi | Read files, validate, transform, load to S3 | âœ… Included |
| **Data Lake** | MinIO (S3-compatible) | Store raw & Iceberg warehouse data | âœ… Included |
| **Transformation** | Apache Spark | Transform raw â†’ bronze (Iceberg) | âœ… Included |
| **Table Format** | Apache Iceberg | ACID transactions, time travel, schema evolution | âœ… Included |
| **Catalog** | Lakekeeper (Iceberg REST) | Manage table metadata | âœ… Included |
| **Metadata Store** | PostgreSQL | Store Airflow metadata + custom lineage | âœ… Included |
| **GitOps** | ArgoCD | Manage K8s manifests | âŒ **Excluded** |
| **Compute Orchestration** | Spark Operator | Submit Spark jobs to K8s | âœ… Included |

---

## 5. Implementation Roadmap

### Phase 1: Infrastructure Setup (Week 1-2)

```bash
Step 1: Kubernetes cluster setup
  â””â”€ kubectl, helm, storageclass

Step 2: Deploy storage layer
  â”œâ”€ PostgreSQL (metadata)
  â”œâ”€ MinIO (data lake)
  â””â”€ Verify bucket creation

Step 3: Deploy message queue
  â”œâ”€ Kafka brokers
  â”œâ”€ Create topics (file-uploaded, raw-data-ready)
  â””â”€ Test connectivity

Step 4: Deploy orchestration
  â”œâ”€ Spark Operator
  â”œâ”€ Airflow
  â””â”€ Configure connections
```

### Phase 2: Data Ingestion (Week 2-3)

```bash
Step 1: Deploy NiFi
  â”œâ”€ Install via Helm
  â”œâ”€ Configure S3 credentials
  â””â”€ Create processor groups

Step 2: Create Datasource API
  â”œâ”€ FastAPI server
  â”œâ”€ File upload endpoint
  â”œâ”€ Kafka producer
  â””â”€ Deploy to K8s

Step 3: Test end-to-end ingestion
  â”œâ”€ Upload file via API
  â”œâ”€ Verify NiFi processing
  â””â”€ Check raw bucket
```

### Phase 3: Spark Transformation (Week 3-4)

```bash
Step 1: Deploy Lakekeeper
  â”œâ”€ Install via Helm
  â”œâ”€ Configure Iceberg catalog
  â””â”€ Create warehouses

Step 2: Develop Spark job
  â”œâ”€ bronze_transform.py
  â”œâ”€ Data quality checks
  â””â”€ Iceberg table creation

Step 3: Create SparkApplication manifest
  â”œâ”€ Template for Airflow
  â”œâ”€ Configure resources
  â””â”€ Test submission

Step 4: Deploy Spark Operator
  â”œâ”€ RBAC setup
  â”œâ”€ Test job submission
  â””â”€ Verify table creation
```

### Phase 4: Airflow Orchestration (Week 4-5)

```bash
Step 1: Develop Airflow DAG
  â”œâ”€ data_ingestion.py
  â”œâ”€ Custom operators
  â”œâ”€ XCom passing
  â””â”€ Error handling

Step 2: Deploy Airflow
  â”œâ”€ Configure executor
  â”œâ”€ Install providers
  â””â”€ Add connections

Step 3: Test complete workflow
  â”œâ”€ Upload file
  â”œâ”€ Monitor Kafka
  â”œâ”€ Submit Spark job
  â”œâ”€ Verify Iceberg table
  â””â”€ Check metrics

Step 4: Setup monitoring
  â”œâ”€ Prometheus
  â”œâ”€ Grafana dashboards
  â””â”€ Alerting
```

### Phase 5: Production Hardening (Week 5-6)

```bash
Step 1: Error handling & retries
  â”œâ”€ Timeout handling
  â”œâ”€ Graceful degradation
  â””â”€ Dead letter queues

Step 2: Data quality framework
  â”œâ”€ Schema validation
  â”œâ”€ Metrics tracking
  â””â”€ Alerting

Step 3: Disaster recovery
  â”œâ”€ Backup strategy
  â”œâ”€ Data recovery procedures
  â””â”€ Runbooks

Step 4: Documentation & training
  â”œâ”€ Setup guide
  â”œâ”€ Troubleshooting
  â””â”€ Operations manual
```

---

## 6. Key Configuration Files

### 6.1 Kafka Topics

```yaml
# infra/k8s/kafka/kafka-topics.yaml

topics:
  - name: file-uploaded
    partitions: 3
    replication_factor: 2
    retention_ms: 604800000  # 7 days
    config:
      compression_type: snappy
      min_insync_replicas: 2
  
  - name: raw-data-ready
    partitions: 3
    replication_factor: 2
    retention_ms: 604800000
    config:
      compression_type: snappy
      min_insync_replicas: 2
  
  - name: bronze-layer-complete
    partitions: 1
    replication_factor: 2
    retention_ms: 2592000000  # 30 days
```

### 6.2 PostgreSQL Schema

```sql
-- Initialize database for Airflow + metrics

-- Job execution metrics
CREATE TABLE public.job_metrics (
    id SERIAL PRIMARY KEY,
    job_id VARCHAR(255) NOT NULL UNIQUE,
    table_name VARCHAR(255),
    input_records INTEGER,
    output_records INTEGER,
    rejected_records INTEGER,
    status VARCHAR(50),
    duration_seconds INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Data quality checks
CREATE TABLE public.data_quality_metrics (
    id SERIAL PRIMARY KEY,
    job_id VARCHAR(255) NOT NULL,
    check_name VARCHAR(255),
    check_status VARCHAR(50),
    check_value TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    FOREIGN KEY (job_id) REFERENCES public.job_metrics(job_id)
);

-- Workflow execution
CREATE TABLE public.workflow_execution (
    id SERIAL PRIMARY KEY,
    workflow_id VARCHAR(255) NOT NULL UNIQUE,
    status VARCHAR(50),
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    total_duration_seconds INTEGER,
    tasks_completed INTEGER,
    tasks_failed INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Table lineage (optional)
CREATE TABLE public.table_lineage (
    id SERIAL PRIMARY KEY,
    source_table VARCHAR(255),
    target_table VARCHAR(255),
    transformation_type VARCHAR(255),
    job_id VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_job_metrics_created_at ON public.job_metrics(created_at);
CREATE INDEX idx_job_metrics_job_id ON public.job_metrics(job_id);
CREATE INDEX idx_data_quality_job_id ON public.data_quality_metrics(job_id);
```

---

## 7. Deployment Commands

```bash
# Setup namespaces
kubectl create namespace data-platform
kubectl create namespace monitoring

# Deploy Kafka
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install kafka bitnami/kafka -f infra/k8s/kafka/helm-values.yaml -n data-platform
bash infra/k8s/kafka/scripts/create_topics.sh

# Deploy MinIO
helm install minio bitnami/minio -f infra/k8s/minio/helm-values.yaml -n data-platform
bash infra/k8s/minio/scripts/create_buckets.sh

# Deploy PostgreSQL
helm install postgresql bitnami/postgresql -f infra/k8s/postgresql/helm-values.yaml -n data-platform
kubectl exec -it postgresql-0 -n data-platform -- psql -U airflow -d airflow -f database-init.sql

# Deploy Lakekeeper
helm install lakekeeper lakekeeper/lakekeeper -f infra/k8s/lakekeeper/helm-values.yaml -n data-platform

# Deploy Spark Operator
helm install spark-operator spark-operator/spark-operator -f infra/k8s/spark-operator/helm-values.yaml -n data-platform

# Deploy NiFi
helm install nifi cetic/nifi -f infra/k8s/nifi/helm-values.yaml -n data-platform

# Deploy Airflow
helm install airflow apache-airflow/airflow -f infra/k8s/airflow/helm-values.yaml -n data-platform

# Verify all components
kubectl get pods -n data-platform
kubectl get svc -n data-platform
```

---

**Architecture Version**: 1.0 (No ArgoCD)  
**Status**: Ready for Implementation  
**Target Deployment**: Kubernetes 1.24+

---

