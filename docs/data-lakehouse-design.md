# Data Lakehouse Architecture vá»›i Kubernetes

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan Kiáº¿n TrÃºc](#1-tá»•ng-quan-kiáº¿n-trÃºc)
2. [CÃ¡c ThÃ nh Pháº§n Chi Tiáº¿t](#2-cÃ¡c-thÃ nh-pháº§n-chi-tiáº¿t)
3. [Data Flow](#3-data-flow)
4. [Triá»ƒn Khai ArgoCD + Spark](#4-triá»ƒn-khai-argocd--spark)
5. [Git Repository Structure](#5-git-repository-structure)
6. [CÃ i Äáº·t & Deployment](#6-cÃ i-Ä‘áº·t--deployment)
7. [Monitoring & Troubleshooting](#7-monitoring--troubleshooting)

---

## 1. Tá»•ng Quan Kiáº¿n TrÃºc

### 1.1 Kiáº¿n TrÃºc Tá»•ng Thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Lakehouse Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTERNAL SERVICES                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Keycloak   â”‚  â”‚   OpenFGA    â”‚  â”‚  Lakekeeper  â”‚  â”‚     MinIO    â”‚   â”‚
â”‚  â”‚ (Auth)       â”‚  â”‚ (AuthZ)      â”‚  â”‚ (Catalog)    â”‚  â”‚ (Storage)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KUBERNETES CLUSTER                                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ DATA INGESTION LAYER                                             â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚
â”‚  â”‚  â”‚ Datasource API      â”‚  â”‚ Kafka Broker                   â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ (Upload endpoint)   â”‚  â”‚ (Publish file metadata)        â”‚   â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
â”‚  â”‚           â”‚                          â–²                         â”‚      â”‚
â”‚  â”‚           â”‚ upload file              â”‚ subscribe               â”‚      â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ ORCHESTRATION LAYER                                              â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚
â”‚  â”‚  â”‚ Airflow DAG         â”‚  â”‚ NiFi Processor                 â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ (Listener + Trigger)â”‚  â”‚ (Data Ingestion Flow)          â”‚   â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  1. Listen to Kafka                                            â”‚      â”‚
â”‚  â”‚  2. Trigger NiFi Flow                                          â”‚      â”‚
â”‚  â”‚  3. Push SparkApplication manifest to Git                      â”‚      â”‚
â”‚  â”‚  4. Trigger ArgoCD Sync                                        â”‚      â”‚
â”‚  â”‚  5. Monitor Spark Job                                          â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ DATA PROCESSING LAYER (GitOps Managed by ArgoCD)                â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚
â”‚  â”‚  â”‚ Spark Operator      â”‚  â”‚ SparkApplication CRDs          â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ (Job Manager)       â”‚  â”‚ (Bronze/Silver/Gold)           â”‚   â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
â”‚  â”‚           â”‚                                                     â”‚      â”‚
â”‚  â”‚           â”‚ Manages                                            â”‚      â”‚
â”‚  â”‚           â–¼                                                     â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚
â”‚  â”‚  â”‚ Spark Driver + Executor Pods                            â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Read from MinIO Raw Bucket                            â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Transform data using Iceberg Catalog (Lakekeeper)    â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Write to MinIO Warehouse (Bronze/Silver/Gold tables) â”‚   â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ CD/GITOPS LAYER                                                  â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚      â”‚
â”‚  â”‚  â”‚ ArgoCD (GitOps Engine)                                   â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Monitor Git Repo                                       â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Sync SparkApplication manifests to K8s                â”‚   â”‚      â”‚
â”‚  â”‚  â”‚ - Self-heal & Automated rollback                        â”‚   â”‚      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚      â”‚
â”‚  â”‚                                                                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA STORAGE (Lakehouse)                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ MinIO (Object Storage)                                          â”‚       â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚
â”‚  â”‚                                                                 â”‚       â”‚
â”‚  â”‚  /raw          (Raw data - Parquet/JSON files)                â”‚       â”‚
â”‚  â”‚    â”œâ”€â”€ file_20250115_001.parquet                              â”‚       â”‚
â”‚  â”‚    â”œâ”€â”€ file_20250115_002.parquet                              â”‚       â”‚
â”‚  â”‚    â””â”€â”€ ...                                                     â”‚       â”‚
â”‚  â”‚                                                                 â”‚       â”‚
â”‚  â”‚  /warehouse    (Iceberg Tables)                               â”‚       â”‚
â”‚  â”‚    â”œâ”€â”€ /bronze (Bronze Layer Tables)                          â”‚       â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ raw_customers (Iceberg table)                      â”‚       â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ raw_orders (Iceberg table)                         â”‚       â”‚
â”‚  â”‚    â”‚   â””â”€â”€ ...                                                 â”‚       â”‚
â”‚  â”‚    â”‚                                                            â”‚       â”‚
â”‚  â”‚    â”œâ”€â”€ /silver (Silver Layer Tables)                          â”‚       â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ customers_clean (Iceberg table)                    â”‚       â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ orders_clean (Iceberg table)                       â”‚       â”‚
â”‚  â”‚    â”‚   â””â”€â”€ ...                                                 â”‚       â”‚
â”‚  â”‚    â”‚                                                            â”‚       â”‚
â”‚  â”‚    â””â”€â”€ /gold (Gold Layer Tables - Analytics Ready)            â”‚       â”‚
â”‚  â”‚        â”œâ”€â”€ customer_metrics (Iceberg table)                   â”‚       â”‚
â”‚  â”‚        â”œâ”€â”€ sales_dashboard (Iceberg table)                    â”‚       â”‚
â”‚  â”‚        â””â”€â”€ ...                                                 â”‚       â”‚
â”‚  â”‚                                                                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                              â”‚
â”‚  Catalog Management:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Lakekeeper (Iceberg REST Catalog)                              â”‚       â”‚
â”‚  â”‚ - Manages table metadata                                       â”‚       â”‚
â”‚  â”‚ - Version control & Time travel                               â”‚       â”‚
â”‚  â”‚ - Access control via OpenFGA                                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GIT REPOSITORY (Single Source of Truth)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  data-lakehouse-gitops/                                                    â”‚
â”‚  â”œâ”€â”€ spark-jobs/                                                           â”‚
â”‚  â”‚   â”œâ”€â”€ bronze-layer/                                                    â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ jobs/                  â† Dynamic jobs (pushed by Airflow)   â”‚
â”‚  â”‚   â”‚   â”‚   â”œâ”€â”€ bronze-dag-run-123.yaml                               â”‚
â”‚  â”‚   â”‚   â”‚   â”œâ”€â”€ bronze-dag-run-124.yaml                               â”‚
â”‚  â”‚   â”‚   â”‚   â””â”€â”€ ...                                                    â”‚
â”‚  â”‚   â”‚   â””â”€â”€ template.yaml          â† Base template                    â”‚
â”‚  â”‚   â”œâ”€â”€ silver-layer/                                                   â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ jobs/                                                      â”‚
â”‚  â”‚   â”‚   â””â”€â”€ template.yaml                                              â”‚
â”‚  â”‚   â””â”€â”€ gold-layer/                                                      â”‚
â”‚  â”‚       â”œâ”€â”€ jobs/                                                       â”‚
â”‚  â”‚       â””â”€â”€ template.yaml                                               â”‚
â”‚  â”‚                                                                        â”‚
â”‚  â”œâ”€â”€ argocd/                                                              â”‚
â”‚  â”‚   â”œâ”€â”€ argocd-app.yaml            â† ArgoCD Application config        â”‚
â”‚  â”‚   â”œâ”€â”€ argocd-cm.yaml             â† ArgoCD ConfigMap                 â”‚
â”‚  â”‚   â””â”€â”€ secret-refs.yaml           â† External Secrets                 â”‚
â”‚  â”‚                                                                        â”‚
â”‚  â””â”€â”€ airflow/                                                             â”‚
â”‚      â”œâ”€â”€ dags/                                                            â”‚
â”‚      â”‚   â””â”€â”€ data_ingestion_gitops.py  â† DAG push manifest to Git     â”‚
â”‚      â””â”€â”€ docker/                                                          â”‚
â”‚          â””â”€â”€ Dockerfile                                                   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Stack Technology

| Component | Purpose | Version |
|-----------|---------|---------|
| **Kubernetes** | Container orchestration | 1.27+ |
| **MinIO** | Object storage (S3 compatible) | latest |
| **Apache Spark** | Distributed data processing | 3.3.0+ |
| **Spark Operator** | Spark job management on K8s | 1.2.0+ |
| **Apache NiFi** | Data ingestion & flow management | 1.18+ |
| **Apache Airflow** | Workflow orchestration | 2.5+ |
| **Apache Kafka** | Event streaming | 3.3+ |
| **Lakekeeper** | Iceberg REST Catalog | latest |
| **Apache Iceberg** | Lakehouse table format | 1.1+ |
| **Keycloak** | Authentication (OIDC) | 20+ |
| **OpenFGA** | Fine-grained authorization | 1.1+ |
| **ArgoCD** | GitOps CD tool | 2.8+ |

---

## 2. CÃ¡c ThÃ nh Pháº§n Chi Tiáº¿t

### 2.1 Datasource API (User Entry Point)

**Má»¥c Ä‘Ã­ch**: API endpoint cho user upload dá»¯ liá»‡u

```python
# datasource_api/main.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from kafka import KafkaProducer
import json
import os
from datetime import datetime

app = FastAPI(title="Datasource API")
kafka_producer = KafkaProducer(
    bootstrap_servers=os.getenv('KAFKA_BROKERS', 'kafka:9092'),
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

@app.post("/api/v1/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    User upload file endpoint
    Payload: nháº­n file tá»« user
    Return: file metadata Ä‘Æ°á»£c gá»­i lÃªn Kafka topic
    """
    try:
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="Filename required")
        
        # Create file metadata (khÃ´ng lÆ°u file, chá»‰ lÆ°u metadata)
        file_metadata = {
            "filename": file.filename,
            "size": file.size,
            "content_type": file.content_type,
            "timestamp": datetime.utcnow().isoformat(),
            "upload_id": f"{datetime.now().timestamp()}_{file.filename}"
        }
        
        # Publish file metadata to Kafka topic
        kafka_producer.send(
            'file-uploaded',
            value=file_metadata
        )
        kafka_producer.flush()
        
        return {
            "status": "success",
            "message": "File metadata published to Kafka",
            "file_metadata": file_metadata
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/health")
async def health_check():
    return {"status": "healthy"}
```

### 2.2 Apache Kafka (Event Bus)

**Má»¥c Ä‘Ã­ch**: Publish file metadata Ä‘á»ƒ Airflow láº¯ng nghe

**Topic Definition:**
```yaml
# Kafka Topic: file-uploaded
Topic: file-uploaded
Partitions: 3
Replication Factor: 2
Retention: 7 days

Message Schema:
{
  "filename": "string",
  "size": "integer",
  "content_type": "string",
  "timestamp": "ISO8601",
  "upload_id": "string"
}
```

### 2.3 Apache NiFi (Data Ingestion)

**Má»¥c Ä‘Ã­ch**: Consume raw data tá»« sources vÃ  lÆ°u vÃ o MinIO raw bucket

```xml
<!-- NiFi Processor Group: RawDataIngestion -->
<!-- Diagram:
ListenHTTP 
  â†“
ConsumeFromKafka (file-uploaded topic)
  â†“
ValidateRecord
  â†“
PutS3Object (MinIO raw bucket)
  â†“
PublishKafkaRecord (raw-data-ready topic)
-->
```

**Key Processors:**
- **ConsumeKafka_2_6**: Consume file metadata tá»« Kafka
- **GetFile**: Fetch actual file data tá»« external source (HTTP/FTP/etc)
- **ValidateRecord**: Validate data format
- **PutS3Object**: Upload file to MinIO `s3://raw/` bucket
- **PublishKafka**: Publish "raw-data-ready" event

### 2.4 Apache Airflow (Orchestration)

**Má»¥c Ä‘Ã­ch**: Orchestrate entire workflow tá»« data ingestion Ä‘áº¿n spark processing

#### 2.4.1 DAG Structure

```python
# airflow/dags/data_ingestion_gitops.py
DAG: data_ingestion_gitops
Schedule: None (triggered by Kafka)
Description: End-to-end data ingestion to lakehouse

Tasks:
1. trigger_nifi_flow          (HttpOperator)
2. wait_raw_data_ready        (BashOperator)
3. push_sparkapp_to_git       (PythonOperator) â† Key task
4. trigger_argocd_sync        (BashOperator)
5. wait_spark_job_complete    (BashOperator)
```

#### 2.4.2 Kafka Listener Mechanism

```python
# Kafka listener for triggering DAG
from airflow.models import Variable
from kafka import KafkaConsumer
import json
import threading

class KafkaDAGTrigger:
    """
    Background thread that listens to Kafka
    and triggers Airflow DAG when new message arrives
    """
    
    def __init__(self):
        self.consumer = KafkaConsumer(
            'file-uploaded',
            bootstrap_servers=['kafka:9092'],
            group_id='airflow-dag-triggers',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
    
    def start_listening(self):
        """Start listening to Kafka messages"""
        for message in self.consumer:
            file_metadata = message.value
            # Trigger DAG with file_metadata as config
            self._trigger_dag(file_metadata)
    
    def _trigger_dag(self, file_metadata):
        """Trigger DAG via REST API"""
        from airflow.api.client.local_client import Client
        client = Client(None, None)
        client.trigger_dag(
            dag_id='data_ingestion_gitops',
            conf=file_metadata
        )

# Start listener in background thread
listener = KafkaDAGTrigger()
listener_thread = threading.Thread(target=listener.start_listening, daemon=True)
listener_thread.start()
```

### 2.5 Spark Operator (Job Execution)

**Má»¥c Ä‘Ã­ch**: Execute Spark jobs on Kubernetes

**Features:**
- Native Kubernetes integration
- Dynamic pod allocation
- Automatic driver/executor provisioning
- Status monitoring

### 2.6 ArgoCD (GitOps CD)

**Má»¥c Ä‘Ã­ch**: Sync SparkApplication manifests tá»« Git vÃ o Kubernetes

**Key Features:**
- Automatic sync tá»« Git repository
- Self-healing (detect drift)
- Rollback via git revert
- Application health monitoring

### 2.7 External Services

#### Keycloak (Authentication)
```yaml
Used by:
- Datasource API: API authentication
- Lakekeeper: User authentication
- Airflow: DAG access control

Config:
OIDC realm: data-lakehouse
Client ID: datasource-api
Redirect URI: https://api.example.com/auth/callback
```

#### OpenFGA (Authorization)
```yaml
Used by:
- Lakekeeper: Table/schema access control
- Spark Jobs: Row/column level authorization

Model:
type user
type dataset
type role

relation member: user -> role
relation can_read: user -> dataset
relation can_write: user -> dataset
```

#### Lakekeeper (Iceberg Catalog)
```yaml
REST Endpoint: http://lakekeeper:8080
Warehouses:
- warehouse_name: prod
  location: s3a://warehouse/

Table Registration:
- Creates metadata in Lakekeeper
- References MinIO storage path
- Version control via Iceberg snapshots
```

---

## 3. Data Flow

### 3.1 End-to-End Flow Diagram

```
TIME: T0
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: USER UPLOAD DATA                                       â”‚
â”‚                                                                â”‚
â”‚  User
â”‚    â”‚
â”‚    â”œâ”€â–¶ POST /api/v1/upload
â”‚    â”‚   Datasource API
â”‚    â”‚     â”‚
â”‚    â”‚     â”œâ”€ Parse file metadata
â”‚    â”‚     â””â”€ Publish to Kafka (file-uploaded topic)
â”‚    â”‚
â”‚  âœ… Response: File metadata published
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 1s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: AIRFLOW KAFKA LISTENER TRIGGERS                        â”‚
â”‚                                                                â”‚
â”‚  Kafka Consumer (Airflow)
â”‚    â”‚
â”‚    â”œâ”€ Consume message from file-uploaded topic
â”‚    â”œâ”€ Extract file metadata
â”‚    â””â”€ Trigger DAG: data_ingestion_gitops
â”‚       Config:
â”‚       {
â”‚         "filename": "orders_20250115.csv",
â”‚         "upload_id": "1234567890_orders_20250115.csv"
â”‚       }
â”‚
â”‚  âœ… DAG Triggered
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 5s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: AIRFLOW TASK 1 - TRIGGER NIFI FLOW                    â”‚
â”‚                                                                â”‚
â”‚  Airflow DAG (Task: trigger_nifi_flow)
â”‚    â”‚
â”‚    â”œâ”€ HttpOperator
â”‚    â”‚  Endpoint: NiFi REST API
â”‚    â”‚  Action: Start RawDataIngestion processor group
â”‚    â”‚
â”‚    â””â”€â–¶ NiFi Processor Group starts:
â”‚          â”œâ”€ ConsumeKafka: Read raw file data
â”‚          â”œâ”€ ValidateRecord: Validate format
â”‚          â”œâ”€ Transform: Convert to Parquet
â”‚          â””â”€ PutS3Object: Upload to MinIO raw bucket
â”‚                         Path: s3://raw/orders_20250115/
â”‚
â”‚  âœ… NiFi Flow Running
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 30s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: AIRFLOW TASK 2 - WAIT FOR RAW DATA                    â”‚
â”‚                                                                â”‚
â”‚  Airflow DAG (Task: wait_raw_data_ready)
â”‚    â”‚
â”‚    â”œâ”€ BashOperator
â”‚    â”‚  Command: Check MinIO for raw data
â”‚    â”‚  Loop until:
â”‚    â”‚    â””â”€ s3://raw/orders_20250115/data.parquet exists
â”‚    â”‚
â”‚    âœ… Raw data ready in MinIO
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 35s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: AIRFLOW TASK 3 - PUSH SPARKAPP TO GIT (CRITICAL!)    â”‚
â”‚                                                                â”‚
â”‚  Airflow DAG (Task: push_sparkapp_to_git)
â”‚    â”‚
â”‚    â”œâ”€ PythonOperator
â”‚    â”‚  Action:
â”‚    â”‚  1. Generate SparkApplication manifest from template
â”‚    â”‚  2. Fill in dynamic parameters:
â”‚    â”‚     - INPUT_PATH: s3://raw/orders_20250115/
â”‚    â”‚     - OUTPUT_PATH: s3://warehouse/bronze/
â”‚    â”‚     - JOB_ID: dag-run-id
â”‚    â”‚  3. Clone Git repo
â”‚    â”‚  4. Commit manifest to:
â”‚    â”‚     spark-jobs/bronze-layer/jobs/bronze-{job_id}.yaml
â”‚    â”‚  5. Push to Git main branch
â”‚    â”‚
â”‚    â””â”€â–¶ Git Repository Updated
â”‚         Path: data-lakehouse-gitops
â”‚         Branch: main
â”‚         File added: spark-jobs/bronze-layer/jobs/bronze-dag-123.yaml
â”‚
â”‚  âœ… SparkApplication manifest in Git
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 40s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: ARGOCD DETECTS GIT CHANGE (AUTOMATIC)                â”‚
â”‚                                                                â”‚
â”‚  ArgoCD Controller
â”‚    â”‚
â”‚    â”œâ”€ Monitor Git repo (default every 3 minutes or webhook)
â”‚    â”œâ”€ Detect new commit in spark-jobs/bronze-layer/jobs/
â”‚    â””â”€ Calculate diff between Git and Kubernetes
â”‚
â”‚  Git vs Kubernetes:
â”‚    Git:         bronze-dag-123.yaml (NEW)
â”‚    Kubernetes:  (MISSING)
â”‚    Status:      OutOfSync
â”‚
â”‚  âœ… Change Detected
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 45s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: ARGOCD SYNCS TO KUBERNETES                            â”‚
â”‚                                                                â”‚
â”‚  ArgoCD Sync Process
â”‚    â”‚
â”‚    â”œâ”€ Read SparkApplication manifest from Git
â”‚    â”œâ”€ kubectl apply -f bronze-dag-123.yaml
â”‚    â””â”€ Create SparkApplication CRD in data-platform namespace
â”‚
â”‚  Kubernetes Cluster:
â”‚    SparkApplication: bronze-layer-dag-123
â”‚    Status: SUBMITTED
â”‚
â”‚  âœ… SparkApplication created in K8s
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 50s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: SPARK OPERATOR PROVISIONS SPARK CLUSTER              â”‚
â”‚                                                                â”‚
â”‚  Spark Operator (watches SparkApplication CRDs)
â”‚    â”‚
â”‚    â”œâ”€ Detect new SparkApplication
â”‚    â”œâ”€ Create Driver Pod:
â”‚    â”‚  spark-driver-bronze-dag-123
â”‚    â”‚  Resources: 2 cores, 2Gi memory
â”‚    â”‚
â”‚    â””â”€ Create Executor Pods (3 instances):
â”‚       spark-exec-1, spark-exec-2, spark-exec-3
â”‚       Resources: 2 cores, 4Gi memory each
â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ Kubernetes Pods Created             â”‚
â”‚  â”‚ â”œâ”€ spark-driver-bronze-dag-123    â”‚
â”‚  â”‚ â”œâ”€ spark-exec-1-bronze-dag-123    â”‚
â”‚  â”‚ â”œâ”€ spark-exec-2-bronze-dag-123    â”‚
â”‚  â”‚ â””â”€ spark-exec-3-bronze-dag-123    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚  Status: RUNNING
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 60s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: SPARK JOB EXECUTES (BRONZE TRANSFORMATION)           â”‚
â”‚                                                                â”‚
â”‚  Spark Driver (bronze-dag-123)
â”‚    â”‚
â”‚    â”œâ”€ Load authentication from Keycloak
â”‚    â”œâ”€ Connect to Lakekeeper (Iceberg Catalog)
â”‚    â”‚  Catalog URI: http://lakekeeper:8080
â”‚    â”‚
â”‚    â”œâ”€ Read from MinIO Raw:
â”‚    â”‚  Path: s3://raw/orders_20250115/data.parquet
â”‚    â”‚  Records: 1,000,000
â”‚    â”‚
â”‚    â”œâ”€ BRONZE Layer Transformation:
â”‚    â”‚  - Data type validation
â”‚    â”‚  - Null value handling
â”‚    â”‚  - Column naming standardization
â”‚    â”‚  - Add metadata columns (loaded_at, source, etc)
â”‚    â”‚
â”‚    â”œâ”€ Write to MinIO Warehouse (Iceberg Table):
â”‚    â”‚  Table: bronze.raw_orders
â”‚    â”‚  Format: Iceberg
â”‚    â”‚  Location: s3://warehouse/bronze/raw_orders/
â”‚    â”‚  Version: v1
â”‚    â”‚
â”‚    â””â”€ Register table in Lakekeeper:
â”‚       Table Name: raw_orders
â”‚       Schema: (order_id, customer_id, amount, loaded_at, ...)
â”‚       Properties: (Iceberg snapshots, partition spec, etc)
â”‚
â”‚  Progress:
â”‚  â”œâ”€ Read:  100%
â”‚  â”œâ”€ Transform: 100%
â”‚  â””â”€ Write: 100%
â”‚
â”‚  âœ… Bronze Layer Complete
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 120s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 10: AIRFLOW TASK 4 - WAIT FOR SPARK JOB                 â”‚
â”‚                                                                â”‚
â”‚  Airflow DAG (Task: wait_spark_complete)
â”‚    â”‚
â”‚    â”œâ”€ BashOperator
â”‚    â”‚  Command: Monitor SparkApplication status
â”‚    â”‚  Loop until: status.applicationState.state == "COMPLETED"
â”‚    â”‚
â”‚    âœ… Spark job completed
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 125s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 11: SILVER LAYER TRANSFORMATION (OPTIONAL)              â”‚
â”‚                                                                â”‚
â”‚  If configured:
â”‚    â”œâ”€ Similar flow to Bronze
â”‚    â”œâ”€ Read from bronze.raw_orders table
â”‚    â”œâ”€ SILVER Layer Transformations:
â”‚    â”‚  - Remove duplicates
â”‚    â”‚  - Business logic enrichment
â”‚    â”‚  - Data quality checks
â”‚    â”‚
â”‚    â””â”€ Write to silver.customers, silver.orders tables
â”‚
â”‚  âœ… Silver Layer Complete
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 150s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 12: GOLD LAYER TRANSFORMATION (OPTIONAL)                â”‚
â”‚                                                                â”‚
â”‚  â”œâ”€ Read from silver tables
â”‚  â”œâ”€ GOLD Layer (Analytics Ready):
â”‚  â”‚  - Aggregated metrics
â”‚  â”‚  - Pre-built dashboards
â”‚  â”‚  - Business KPIs
â”‚  â”‚
â”‚  â””â”€ Write to gold.customer_metrics, gold.sales_dashboard tables
â”‚
â”‚  âœ… All Layers Complete
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME: T0 + 155s
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FINAL STATE: DATA LAKEHOUSE READY                             â”‚
â”‚                                                                â”‚
â”‚  MinIO Warehouse Structure:
â”‚  /warehouse
â”‚  â”œâ”€â”€ /bronze
â”‚  â”‚   â”œâ”€â”€ raw_orders/     â† Loaded & transformed
â”‚  â”‚   â””â”€â”€ raw_customers/  â† Loaded & transformed
â”‚  â”‚
â”‚  â”œâ”€â”€ /silver
â”‚  â”‚   â”œâ”€â”€ customers/      â† Cleaned & enriched
â”‚  â”‚   â””â”€â”€ orders/         â† Cleaned & enriched
â”‚  â”‚
â”‚  â””â”€â”€ /gold
â”‚      â”œâ”€â”€ customer_metrics/    â† Aggregated
â”‚      â””â”€â”€ sales_dashboard/     â† Pre-built
â”‚
â”‚  Lakekeeper Catalog:
â”‚  â””â”€â”€ Tables registered with Iceberg metadata
â”‚      - Version history maintained
â”‚      - Time travel enabled
â”‚      - Access control via OpenFGA
â”‚
â”‚  âœ… WORKFLOW COMPLETE - Data ready for analytics!
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Git Flow trong Kiáº¿n TrÃºc

```
                 Developer / CI
                      â”‚
                      â”‚ Write code
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Git Repository (Main)     â”‚
        â”‚                           â”‚
        â”‚ spark-jobs/               â”‚
        â”‚ â”œâ”€â”€ bronze-layer/         â”‚
        â”‚ â”‚   â””â”€â”€ jobs/             â”‚
        â”‚ â”‚       â”œâ”€â”€ bronze-123.yaml
        â”‚ â”‚       â”œâ”€â”€ bronze-124.yaml
        â”‚ â”‚       â””â”€â”€ bronze-125.yaml
        â”‚ â””â”€â”€ argocd/               â”‚
        â”‚     â””â”€â”€ argocd-app.yaml   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²
                      â”‚ Git push
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Airflow Task 3            â”‚
        â”‚ push_sparkapp_to_git      â”‚
        â”‚                           â”‚
        â”‚ Actions:                  â”‚
        â”‚ 1. Clone repo             â”‚
        â”‚ 2. Generate YAML          â”‚
        â”‚ 3. Commit manifest        â”‚
        â”‚ 4. Push to main           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–²
                      â”‚ Manifest generated
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Airflow DAG Execution     â”‚
        â”‚                           â”‚
        â”‚ Task 1: Trigger NiFi      â”‚
        â”‚ Task 2: Wait Raw Data     â”‚
        â”‚ Task 3: Push to Git â—„â”€â”€â”  â”‚
        â”‚ Task 4: Sync ArgoCD  â”‚  â”‚
        â”‚ Task 5: Monitor Job  â”‚  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                      â–²           â”‚
                      â”‚           â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Triá»ƒn Khai ArgoCD + Spark

### 4.1 ArgoCD Flow Chi Tiáº¿t

```
Git Repository (Source of Truth)
        â”‚
        â”‚ Webhook / Polling (every 3min)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ArgoCD Application Controller    â”‚
â”‚                                 â”‚
â”‚ Monitor:                        â”‚
â”‚ - Git repo: main branch         â”‚
â”‚ - Path: spark-jobs/bronze-layer/
â”‚ - Files: *.yaml                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ Detect new/changed manifest
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Diff Calculator                 â”‚
â”‚                                 â”‚
â”‚ Compare:                        â”‚
â”‚ Git:        bronze-123.yaml     â”‚
â”‚ Kubernetes: (missing)           â”‚
â”‚                                 â”‚
â”‚ Status: OutOfSync               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ Auto sync (if enabled)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes API                  â”‚
â”‚                                 â”‚
â”‚ kubectl apply -f bronze-123.yamlâ”‚
â”‚                                 â”‚
â”‚ Create SparkApplication CRD:    â”‚
â”‚ - Name: bronze-layer-123       â”‚
â”‚ - Namespace: data-platform     â”‚
â”‚ - Spec: From Git manifest      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ CRD created
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Operator (watches CRD)    â”‚
â”‚                                 â”‚
â”‚ Actions:                        â”‚
â”‚ 1. Detect new SparkApplication â”‚
â”‚ 2. Validate spec                â”‚
â”‚ 3. Create Driver Pod            â”‚
â”‚ 4. Create Executor Pods (3x)    â”‚
â”‚ 5. Monitor execution            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ Pods created
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kubernetes Scheduler            â”‚
â”‚                                 â”‚
â”‚ Actions:                        â”‚
â”‚ 1. Schedule pods to nodes       â”‚
â”‚ 2. Pull docker images           â”‚
â”‚ 3. Start containers             â”‚
â”‚ 4. Mount volumes (MinIO creds)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ Containers started
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Job Execution             â”‚
â”‚                                 â”‚
â”‚ Driver Pod:                     â”‚
â”‚ - Load Spark config             â”‚
â”‚ - Connect to Lakekeeper         â”‚
â”‚ - Read from MinIO               â”‚
â”‚ - Coordinate executors          â”‚
â”‚ - Write results                 â”‚
â”‚                                 â”‚
â”‚ Executor Pods (3x):             â”‚
â”‚ - Process partitions in parallelâ”‚
â”‚ - Push data to driver           â”‚
â”‚ - Return results                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ Job completed
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Iceberg Tables                  â”‚
â”‚                                 â”‚
â”‚ MinIO Warehouse:                â”‚
â”‚ s3://warehouse/bronze/          â”‚
â”‚ â””â”€â”€ raw_orders/                 â”‚
â”‚     â”œâ”€â”€ metadata/               â”‚
â”‚     â””â”€â”€ data/                   â”‚
â”‚                                 â”‚
â”‚ Lakekeeper Catalog:             â”‚
â”‚ - Table: bronze.raw_orders      â”‚
â”‚ - Version: 1                    â”‚
â”‚ - Snapshots: metadata cached    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Git Repository Structure

```
data-lakehouse-gitops/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ spark-jobs/                          # â† Main focus for ArgoCD
â”‚   â”œâ”€â”€ kustomization.yaml               # Base Kustomize config
â”‚   â”‚
â”‚   â”œâ”€â”€ bronze-layer/
â”‚   â”‚   â”œâ”€â”€ template.yaml                # Template (not deployed directly)
â”‚   â”‚   â”œâ”€â”€ values.yaml                  # Default values
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml           # Kustomize overlay
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ jobs/                        # â† Dynamic jobs pushed by Airflow
â”‚   â”‚       â”œâ”€â”€ bronze-dag-run-123.yaml
â”‚   â”‚       â”œâ”€â”€ bronze-dag-run-124.yaml
â”‚   â”‚       â””â”€â”€ bronze-dag-run-125.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ silver-layer/
â”‚   â”‚   â”œâ”€â”€ template.yaml
â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ jobs/
â”‚   â”‚       â”œâ”€â”€ silver-dag-run-123.yaml
â”‚   â”‚       â””â”€â”€ silver-dag-run-124.yaml
â”‚   â”‚
â”‚   â””â”€â”€ gold-layer/
â”‚       â”œâ”€â”€ template.yaml
â”‚       â”œâ”€â”€ values.yaml
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ jobs/
â”‚           â”œâ”€â”€ gold-dag-run-123.yaml
â”‚           â””â”€â”€ gold-dag-run-124.yaml
â”‚
â”œâ”€â”€ argocd/                              # ArgoCD configs
â”‚   â”œâ”€â”€ argocd-app.yaml                  # ArgoCD Application manifest
â”‚   â”œâ”€â”€ argocd-appset.yaml               # Optional: ApplicationSet
â”‚   â”œâ”€â”€ argocd-cm.yaml                   # ArgoCD ConfigMap
â”‚   â”œâ”€â”€ argocd-rbac.yaml                 # RBAC policies
â”‚   â”œâ”€â”€ notification-secret.yaml         # Notification configs
â”‚   â””â”€â”€ sync-strategy.yaml               # Advanced sync strategies
â”‚
â”œâ”€â”€ airflow/                             # Airflow DAG & configs
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ data_ingestion_gitops.py     # Main DAG
â”‚   â”‚   â”œâ”€â”€ spark_bronze_layer.py        # Bronze specific (optional)
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ git_utils.py             # Git operations
â”‚   â”‚       â””â”€â”€ k8s_utils.py             # K8s operations
â”‚   â”‚
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ kubernetes/                          # K8s cluster configs
â”‚   â”œâ”€â”€ namespaces/
â”‚   â”‚   â”œâ”€â”€ data-platform.yaml
â”‚   â”‚   â”œâ”€â”€ airflow.yaml
â”‚   â”‚   â””â”€â”€ argocd.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ secrets/
â”‚   â”‚   â”œâ”€â”€ minio-credentials.yaml
â”‚   â”‚   â”œâ”€â”€ github-token.yaml            # For Airflow to push to Git
â”‚   â”‚   â”œâ”€â”€ keycloak-credentials.yaml
â”‚   â”‚   â””â”€â”€ lakekeeper-config.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ rbac/
â”‚   â”‚   â”œâ”€â”€ spark-serviceaccount.yaml
â”‚   â”‚   â””â”€â”€ airflow-rbac.yaml
â”‚   â”‚
â”‚   â””â”€â”€ configmaps/
â”‚       â”œâ”€â”€ spark-config.yaml
â”‚       â””â”€â”€ nifi-config.yaml
â”‚
â”œâ”€â”€ helm/                                # Helm charts (optional)
â”‚   â”œâ”€â”€ spark-operator/
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ argocd/
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ SETUP.md
    â”œâ”€â”€ TROUBLESHOOTING.md
    â””â”€â”€ BACKUP_RESTORE.md
```

### 4.3 SparkApplication Manifest Template

```yaml
# spark-jobs/bronze-layer/template.yaml
# This is a TEMPLATE - Airflow will generate actual instances from this

apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: bronze-layer-{{ job_id }}        # Dynamically filled by Airflow
  namespace: data-platform
  labels:
    app: data-lakehouse
    layer: bronze
    job-id: "{{ job_id }}"
  annotations:
    argocd.argoproj.io/compare-result: "true"

spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: spark:3.3.0-scala2.12
  imagePullPolicy: IfNotPresent
  
  # Main Spark application
  mainApplicationFile: "s3a://spark-scripts/bronze_transform.py"
  
  # Script args - passed to main()
  arguments:
    - "--input-path"
    - "{{ input_path }}"        # Airflow fills: s3a://raw/file_name/
    - "--output-path"
    - "{{ output_path }}"       # Airflow fills: s3a://warehouse/bronze/
    - "--job-id"
    - "{{ job_id }}"
    - "--catalog-uri"
    - "{{ catalog_uri }}"       # Airflow fills: http://lakekeeper:8080
  
  sparkVersion: "3.3.0"
  
  # Restart policy for failures
  restartPolicy:
    type: OnFailure
    onFailureRetries: 2
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 1
    onSubmissionFailureRetryInterval: 20
  
  # Driver pod configuration
  driver:
    cores: 2
    memory: 2Gi
    memoryOverhead: 256m
    labels:
      version: v3.3.0
    serviceAccount: spark-operator
    
    # Environment variables from secrets
    env:
      - name: MINIO_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-credentials
            key: access-key
      - name: MINIO_SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio-credentials
            key: secret-key
      - name: KEYCLOAK_URL
        valueFrom:
          configMapKeyRef:
            name: external-services
            key: keycloak-url
    
    # Node affinity - run on specific nodes
    nodeSelector:
      workload: data-processing
    
    # Tolerations for taints
    tolerations:
      - key: "data-processing"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
  
  # Executor pod configuration
  executor:
    cores: 2
    instances: 3
    memory: 4Gi
    memoryOverhead: 512m
    
    env:
      - name: MINIO_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-credentials
            key: access-key
      - name: MINIO_SECRET_KEY
        valueFrom:
          secretKeyRef:
            name: minio-credentials
            key: secret-key
    
    nodeSelector:
      workload: data-processing
    
    tolerations:
      - key: "data-processing"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
  
  # Spark configuration
  sparkConf:
    # Iceberg catalog configuration
    "spark.sql.warehouse.dir": "s3a://warehouse"
    "spark.sql.catalog.iceberg": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.iceberg.type": "rest"
    "spark.sql.catalog.iceberg.uri": "{{ catalog_uri }}"
    "spark.sql.catalog.iceberg.s3.endpoint": "{{ minio_endpoint }}"
    
    # S3/MinIO configuration
    "spark.hadoop.fs.s3a.endpoint": "{{ minio_endpoint }}"
    "spark.hadoop.fs.s3a.access.key": "${MINIO_ACCESS_KEY}"
    "spark.hadoop.fs.s3a.secret.key": "${MINIO_SECRET_KEY}"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
    
    # Performance tuning
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.skewJoin.enabled": "true"
    "spark.sql.shuffle.partitions": "200"
    
    # Logging
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://spark-logs/events"
  
  # Hadoop configuration
  hadoopConf:
    "fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
  
  # Volumes and volume mounts
  volumes:
    - name: spark-scripts
      configMap:
        name: spark-bronze-scripts
    - name: py-requirements
      configMap:
        name: spark-requirements
  
  # Security context
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
```

### 4.4 ArgoCD Application Manifest

```yaml
# argocd/argocd-app.yaml

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: data-lakehouse-spark-jobs
  namespace: argocd
  
  # Finalizers ensure proper cleanup
  finalizers:
    - resources-finalizer.argocd.argoproj.io

spec:
  # ArgoCD project
  project: default
  
  # Source: Git repository
  source:
    repoURL: https://github.com/your-org/data-lakehouse-gitops.git
    targetRevision: main
    path: spark-jobs
    
    # Using Kustomize for manifest generation
    kustomize:
      version: v5.0.0
      # Don't automatically apply Kustomize
      # We manage kustomization.yaml explicitly
  
  # Destination: Kubernetes cluster
  destination:
    server: https://kubernetes.default.svc
    namespace: data-platform
  
  # Sync policy
  syncPolicy:
    # Automatic syncing
    automated:
      prune: true       # Delete K8s resources not in Git
      selfHeal: true    # Auto-sync when K8s drifts from Git
      allow:
        empty: false    # Prevent syncing empty repos
    
    # Sync options
    syncOptions:
      - CreateNamespace=true
      - RespectIgnoreDifferences=true
    
    # Retry policy for failed syncs
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # Ignore differences in status fields
  ignoreDifferences:
    - group: sparkoperator.k8s.io
      kind: SparkApplication
      jsonPointers:
        - /status              # Ignore status changes
        - /metadata/generation # Ignore generation changes
  
  # Health assessment
  statusBadgeEnabled: true
```

---

## 5. CÃ i Äáº·t & Deployment

### 5.1 Prerequisites

```bash
# Cluster requirements
- Kubernetes 1.27+
- kubectl configured
- Helm 3.10+
- Git account with token
- MinIO access credentials

# Tools needed
- git
- kubectl
- helm
- argocd CLI (optional)
- spark-submit (for local testing)
```

### 5.2 Installation Steps

#### Step 1: Create Kubernetes Namespaces

```bash
kubectl create namespace data-platform
kubectl create namespace argocd
kubectl create namespace airflow
kubectl create namespace spark-operator
```

#### Step 2: Install Spark Operator

```bash
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm repo update

helm install spark-operator spark-operator/spark-operator \
  --namespace spark-operator \
  --set sparkJobNamespace=data-platform \
  --set enableWebhook=true
```

#### Step 3: Install ArgoCD

```bash
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Wait for ArgoCD to be ready
kubectl wait --for=condition=ready pod \
  -l app.kubernetes.io/name=argocd-server \
  -n argocd --timeout=300s

# Get initial admin password
argocd admin initial-password -n argocd
```

#### Step 4: Configure ArgoCD Git Access

```bash
# Create GitHub token secret
kubectl create secret generic github-credentials \
  -n argocd \
  --from-literal=username=your-github-user \
  --from-literal=password=your-github-token

# Configure repository in ArgoCD
argocd repo add https://github.com/your-org/data-lakehouse-gitops.git \
  --username your-github-user \
  --password your-github-token
```

#### Step 5: Create Kubernetes Secrets

```bash
# MinIO credentials
kubectl create secret generic minio-credentials \
  -n data-platform \
  --from-literal=access-key=minioadmin \
  --from-literal=secret-key=minioadmin \
  --from-literal=endpoint=http://minio:9000

# GitHub token for Airflow to push
kubectl create secret generic github-push-token \
  -n airflow \
  --from-literal=token=your-github-token \
  --from-literal=user=your-github-user
```

#### Step 6: Install ArgoCD Application

```bash
kubectl apply -f argocd/argocd-app.yaml -n argocd

# Verify
kubectl get application -n argocd
kubectl get applications data-lakehouse-spark-jobs -n argocd -o yaml
```

#### Step 7: Install Airflow

```bash
# Using Helm
helm repo add apache-airflow https://airflow.apache.org
helm repo update

helm install airflow apache-airflow/airflow \
  -n airflow \
  -f airflow/values.yaml

# Or using Docker Compose for testing
docker-compose -f airflow/docker-compose.yaml up
```

#### Step 8: Deploy Kafka

```bash
# Simple Kafka using Docker Compose (for dev)
docker-compose -f kafka/docker-compose.yaml up

# Or using Helm (for production)
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install kafka bitnami/kafka \
  -n data-platform \
  -f kafka/values.yaml
```

---

## 6. Monitoring & Troubleshooting

### 6.1 Monitoring Tools

#### Check ArgoCD Sync Status

```bash
# List all applications
kubectl get applications -n argocd

# Describe specific application
kubectl describe application data-lakehouse-spark-jobs -n argocd

# Watch sync status
kubectl get application data-lakehouse-spark-jobs \
  -n argocd -w -o json | jq '.status'

# ArgoCD UI
argocd login localhost:8080
argocd app get data-lakehouse-spark-jobs
```

#### Monitor Spark Jobs

```bash
# List SparkApplications
kubectl get sparkapplications -n data-platform

# Watch job status
kubectl get sparkapplication bronze-layer-dag-123 \
  -n data-platform -w -o json

# View pod logs
kubectl logs spark-driver-bronze-dag-123 -n data-platform
kubectl logs spark-exec-1-bronze-dag-123 -n data-platform

# Spark UI (port-forward)
kubectl port-forward spark-driver-bronze-dag-123 4040:4040 -n data-platform
# Access: http://localhost:4040
```

#### Monitor Airflow DAG

```bash
# Check DAG status
airflow dags list
airflow dags info data_ingestion_gitops

# Check DAG runs
airflow dags list-runs -d data_ingestion_gitops

# View task logs
airflow tasks log data_ingestion_gitops push_sparkapp_to_git 2025-01-15T10:00:00
```

### 6.2 Common Issues & Solutions

#### Issue 1: ArgoCD Not Syncing

```bash
# Check ArgoCD controller logs
kubectl logs -n argocd deployment/argocd-application-controller

# Manually trigger sync
argocd app sync data-lakehouse-spark-jobs

# Force sync
argocd app sync data-lakehouse-spark-jobs --force

# Check Git connection
argocd repo list
```

#### Issue 2: SparkApplication Stuck in SUBMITTED

```bash
# Check Spark Operator logs
kubectl logs -n spark-operator deployment/spark-operator

# Check Pod Events
kubectl describe pod spark-driver-bronze-dag-123 -n data-platform

# Check resource availability
kubectl describe nodes

# Check RBAC permissions
kubectl auth can-i create sparkapplications --as=system:serviceaccount:spark-operator:spark-operator
```

#### Issue 3: MinIO Connectivity Issues

```bash
# Test MinIO connectivity from pod
kubectl run -it --rm debug --image=minio/mc:latest -n data-platform -- \
  bash -c "mc alias set minio http://minio:9000 minioadmin minioadmin && mc ls minio/raw"

# Check credentials
kubectl get secret minio-credentials -n data-platform -o yaml
```

#### Issue 4: Iceberg Catalog Errors

```bash
# Test Lakekeeper connectivity
kubectl run -it --rm debug --image=curlimages/curl -n data-platform -- \
  curl http://lakekeeper:8080/api/v1/config

# Check table metadata
curl http://localhost:8080/api/v1/namespaces/bronze

# Verify table registration
spark-sql --packages org.apache.iceberg:iceberg-spark-runtime_2.12:1.1.0 \
  --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.iceberg.type=rest \
  --conf spark.sql.catalog.iceberg.uri=http://lakekeeper:8080 \
  -e "USE iceberg; SHOW TABLES;"
```

### 6.3 Monitoring Dashboard Setup

#### Prometheus + Grafana for Spark Metrics

```yaml
# monitoring/prometheus-scrape-spark.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-spark-config
  namespace: monitoring
data:
  spark-targets.json: |
    [
      {
        "targets": ["spark-driver-*:4040"],
        "labels": {
          "job": "spark-driver",
          "application": "bronze-layer"
        }
      }
    ]
```

#### Alert Rules

```yaml
# monitoring/alert-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: spark-alerts
  namespace: monitoring
spec:
  groups:
    - name: spark.rules
      rules:
        - alert: SparkJobFailure
          expr: spark_app_state{state="FAILED"} == 1
          for: 5m
          annotations:
            summary: "Spark job {{ $labels.app }} failed"
        
        - alert: SparkExecutorDown
          expr: spark_executor_memory_used_bytes offset 5m - spark_executor_memory_used_bytes > 0
          for: 5m
          annotations:
            summary: "Spark executor {{ $labels.executor }} is down"
```

---

## 7. Best Practices & Recommendations

### 7.1 GitOps Best Practices

```yaml
# âœ… DO: Commit SparkApplication specs to Git
âœ“ Version control all configurations
âœ“ Use meaningful commit messages
âœ“ Require PR reviews for changes
âœ“ Tag releases for production deployments
âœ“ Keep Git repo as single source of truth

# âŒ DON'T: Apply K8s manifests directly
âœ— kubectl apply -f sparkapp.yaml (directly to prod)
âœ— Edit secrets directly in cluster
âœ— Manual configuration changes
âœ— Skip Git commit for quick fixes
```

### 7.2 Spark Job Best Practices

```python
# âœ… DO: Use Iceberg table format
âœ“ ACID transactions
âœ“ Time travel / snapshots
âœ“ Schema evolution
âœ“ Partition evolution

# âœ… DO: Optimize Spark configuration
âœ“ Proper partition count
âœ“ Enable adaptive query execution
âœ“ Tune memory allocation based on data size
âœ“ Use file format compression (Parquet)

# âœ… DO: Add data quality checks
âœ“ Validate record counts
âœ“ Check null percentages
âœ“ Validate data types
âœ“ Compare min/max values

# âŒ DON'T: Write directly to gold layer
âœ— Skip intermediate layers (bronze/silver)
âœ— Store raw data in gold
```

### 7.3 Security Best Practices

```yaml
# âœ… DO: Use Kubernetes Secrets for credentials
âœ“ Store credentials in K8s Secrets
âœ“ Use External Secrets Operator for encryption
âœ“ Enable RBAC for service accounts
âœ“ Use network policies to restrict traffic

# âœ… DO: Enable authentication & authorization
âœ“ Keycloak for API authentication
âœ“ OpenFGA for fine-grained access control
âœ“ Lakekeeper enforces table-level access

# âŒ DON'T: Hardcode credentials
âœ— Credentials in code/configs
âœ— Default passwords in production
âœ— World-readable secrets
```

### 7.4 Performance Best Practices

```yaml
# âœ… DO: Use Kubernetes node pools
âœ“ Dedicated nodes for Spark jobs
âœ“ Use node selectors & taints/tolerations
âœ“ Reserve resources properly
âœ“ Use cluster autoscaling

# âœ… DO: Optimize data movement
âœ“ Partition data logically
âœ“ Use columnar formats (Parquet)
âœ“ Compress intermediate outputs
âœ“ Minimize network traffic

# âœ… DO: Monitor resource usage
âœ“ Set resource limits
âœ“ Monitor pod evictions
âœ“ Track disk I/O
âœ“ Alert on resource exhaustion
```

---

## 8. Conclusion

### 8.1 Architecture Summary

```
User Upload
    â†“
Datasource API + Kafka
    â†“
Airflow (Orchestration)
    â†“
    â”œâ”€ Trigger NiFi (Ingestion) â†’ MinIO Raw Bucket
    â”‚
    â””â”€ Push SparkApplication to Git
         â†“
      ArgoCD (GitOps)
         â†“
      Kubernetes
         â†“
      Spark Operator
         â†“
      Spark Cluster
         â†“
      Transform Data
         â†“
      MinIO Warehouse (Iceberg Tables)
         â†“
      Ready for Analytics
```

### 8.2 Key Benefits

| Aspect | Benefit |
|--------|---------|
| **Scalability** | Kubernetes auto-scaling handles variable workloads |
| **Reliability** | Multi-replica setup with automatic failover |
| **Auditability** | Git history provides complete audit trail |
| **Recoverability** | Iceberg time-travel enables data rollback |
| **Flexibility** | Easy to add new transformation layers |
| **Cost Efficiency** | Use resources only when needed |
| **Compliance** | Fine-grained access control via OpenFGA |

### 8.3 Next Steps

1. **Setup Git Repository** - Clone template, customize for your org
2. **Deploy K8s Cluster** - Spin up Kubernetes (EKS/GKE/AKS)
3. **Install Components** - Follow installation steps in section 5
4. **Test Locally** - Use docker-compose for initial testing
5. **Setup CI/CD** - Configure GitHub Actions for automated testing
6. **Monitor & Alert** - Setup Prometheus + Grafana + PagerDuty
7. **Optimize** - Fine-tune resource allocations based on metrics
8. **Scale Out** - Add more executors, additional transformation layers

---

## References

- [Apache Iceberg Documentation](https://iceberg.apache.org/)
- [Spark Operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)
- [ArgoCD Documentation](https://argo-cd.readthedocs.io/)
- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/overview/)
- [Apache Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)

---

**Document Version**: 1.0  
**Last Updated**: 2025-01-15  
**Author**: Data Engineering Team
